理解您的需求后，我们将详细调整之前的训练过程，以适应新的词汇表配置。新的配置如下：

- **源语言有效词汇数量**：60（不包含特殊标识符 `<pad>`, `<sos>`, `<eos>`, `<unk>`）
- **目标语言有效词汇数量**：35（不包含特殊标识符 `<pad>`, `<sos>`, `<eos>`, `<unk>`）
- **特殊标识符**：分别为 `<pad>`, `<sos>`, `<eos>`, `<unk>`，每个语言各自独立管理。

我们将分步完成以下内容：

1. **重新构建词汇表**
2. **调整数据预处理**
3. **调整模型定义**
4. **定义损失函数**
5. **准备数据加载器**
6. **训练循环**
7. **保存和加载模型**

以下将详细介绍每一步，并提供相应的代码示例。

---

## **1. 重新构建词汇表**

首先，我们需要为源语言和目标语言分别构建词汇表，并包含必要的特殊标识符。由于有效词汇量已明确为60和35，因此词汇表大小将分别为64（60 + 4）和39（35 + 4）。

### **1.1 定义特殊标识符**

```python
# 定义特殊标识符
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']
```

### **1.2 构建源语言和目标语言词汇表**

假设您已经有源语言和目标语言的有效词汇列表：

```python
# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词


# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词
common_nouns = [
    "time", "person", "year", "way", "day", "thing", "man", "world", "life", "hand",
    "part", "child", "eye", "woman", "place", "work", "week", "case", "point", "government",
    "company", "number", "group", "problem", "fact", "be", "have", "do", "say", "get",
    "make", "go", "know", "take", "see"
]
noun_index_dict = {noun: index for index, noun in enumerate(common_nouns, start=1)}

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

# 检查词汇表大小
print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39
```

**输出：**
```
Source Vocabulary Size: 64
Target Vocabulary Size: 39
```

---

## **2. 调整数据预处理**

在训练前，需要将文本数据转换为模型可接受的格式。这包括分词、编码，以及生成相应的掩码。

### **2.1 分词与编码函数**

```python
def tokenize_and_encode(sentence, vocab):
    """
    简单的分词与编码函数
    :param sentence: str, 输入句子
    :param vocab: dict, 词汇表 {word: id}
    :return: list of int, 编码后的ID序列
    """
    tokens = sentence.strip().split()
    tokens = ['<sos>'] + tokens + ['<eos>']
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    return ids
```

### **2.2 创建填充掩码**

填充掩码用于标识序列中的填充符，以防止模型在计算注意力时关注这些位置。

```python
def create_padding_mask(seq, pad_idx):
    """
    创建填充掩码
    :param seq: Tensor, [batch_size, seq_len]
    :param pad_idx: int, 填充符的ID
    :return: Tensor, [batch_size, 1, 1, seq_len]
    """
    mask = (seq == pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
    return mask
```

### **2.3 示例数据预处理**

假设您有一批源语言和目标语言句子：

```python
# 示例句子
source_sentences = [
    "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5",
    "src_word_6 src_word_7 src_word_8 src_word_9 src_word_10",
    # ... 更多句子
]

target_sentences = [
    "tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5",
    "tgt_word_6 tgt_word_7 tgt_word_8 tgt_word_9 tgt_word_10",
    # ... 更多句子
]

# 编码句子
encoded_sources = [tokenize_and_encode(sent, src_vocab) for sent in source_sentences]
encoded_targets = [tokenize_and_encode(sent, tgt_vocab) for sent in target_sentences]

# 假设使用PyTorch的pad_sequence进行填充
from torch.nn.utils.rnn import pad_sequence

# 转换为Tensor
source_tensors = [torch.tensor(seq, dtype=torch.long) for seq in encoded_sources]
target_tensors = [torch.tensor(seq, dtype=torch.long) for seq in encoded_targets]

# 填充序列
source_padded = pad_sequence(source_tensors, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
target_padded = pad_sequence(target_tensors, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]

print(source_padded.shape)
print(target_padded.shape)
```

**注意：**
- 根据您的数据集，可能需要调整批次大小和序列长度。
- 若句子长度不统一，`pad_sequence` 确保了批次中所有序列具有相同长度。

---

## **3. 调整模型定义**

根据新的词汇表配置，调整Transformer模型的嵌入层和输出层。

### **3.1 定义位置编码**

位置编码用于注入位置信息，使模型能够理解序列中词汇的顺序。

```python
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        :param x: Tensor, [batch_size, seq_len, d_model]
        :return: Tensor, 加入位置信息后的输入
        """
        x = x + self.pe[:, :x.size(1)]
        return x
```

### **3.2 定义Transformer模型**

确保嵌入层和输出层的词汇表大小与新的配置一致。

```python
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        """
        生成目标掩码
        :param sz: int, 序列长度
        :return: Tensor, [sz, sz]
        """
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        """
        :param src: Tensor, [batch_size, src_seq_len]
        :param tgt: Tensor, [batch_size, tgt_seq_len]
        :param src_mask: Tensor, [src_seq_len, src_seq_len]
        :param tgt_mask: Tensor, [tgt_seq_len, tgt_seq_len]
        :param src_padding_mask: Tensor, [batch_size, src_seq_len]
        :param tgt_padding_mask: Tensor, [batch_size, tgt_seq_len]
        :param memory_key_padding_mask: Tensor, [batch_size, src_seq_len]
        :return: Tensor, [batch_size, tgt_seq_len, tgt_vocab_size]
        """
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)  # [batch_size, src_seq_len, d_model]
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)  # [batch_size, tgt_seq_len, d_model]
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits
```

### **3.3 初始化并加载模型**

```python
import torch

# 参数定义
src_vocab_size = len(src_vocab)  # 64
tgt_vocab_size = len(tgt_vocab)  # 39
d_model = 512
n_head = 8
num_encoder_layers = 6
num_decoder_layers = 6
dim_feedforward = 2048
dropout = 0.1
max_len = 100

# 实例化模型
model = TransformerModel(
    src_vocab_size=src_vocab_size, 
    tgt_vocab_size=tgt_vocab_size, 
    d_model=d_model, 
    n_head=n_head, 
    num_encoder_layers=num_encoder_layers, 
    num_decoder_layers=num_decoder_layers, 
    dim_feedforward=dim_feedforward, 
    dropout=dropout,
    max_len=max_len
)

# 检查设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# 加载训练好的模型参数（假设文件名为 'best_model.pt'）
model.load_state_dict(torch.load('best_model.pt', map_location=device))
model.eval()  # 设置为评估模式
```

**注意：**
- 确保模型定义与训练时完全一致，否则可能会导致参数加载错误。

---

## **4. 定义损失函数**

为了满足您的需求，即“历史预测中出现次数多的词，当前预测时概率较小”，我们将使用**交叉熵损失**，并在其中引入**频率权重**，以降低频繁词的损失权重。

### **4.1 频率加权交叉熵损失**

```python
import torch.nn as nn
import torch.nn.functional as F

class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失（忽略<pad>）
        return loss.mean()
```

**解释：**
- `history`：解码器当前批次的输入目标序列，用于计算历史中词的频率。
- `penalty_factor`：控制频率惩罚力度，需根据验证集表现进行调整。
- `ignore_index`：填充符的ID，在本例中为`<pad>`对应的ID。

---

## **5. 准备数据加载器**

使用PyTorch的`DataLoader`来批量加载训练数据。假设您已经有自己的数据集类或使用现有的数据加载方式。

### **5.1 定义自定义Dataset**

```python
from torch.utils.data import Dataset, DataLoader

class TranslationDataset(Dataset):
    def __init__(self, source_sentences, target_sentences, src_vocab, tgt_vocab):
        """
        :param source_sentences: list of str, 源语言句子
        :param target_sentences: list of str, 目标语言句子
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sentences) == len(target_sentences), "源语言和目标语言句子数量不一致"
        self.source_sentences = source_sentences
        self.target_sentences = target_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sentences)
    
    def __getitem__(self, idx):
        src_seq = tokenize_and_encode(self.source_sentences[idx], self.src_vocab)
        tgt_seq = tokenize_and_encode(self.target_sentences[idx], self.tgt_vocab)
        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)
```

### **5.2 定义collate_fn**

`collate_fn`用于在批处理中对不同长度的序列进行填充。

```python
def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标序列及对应的掩码
    """
    src_batch, tgt_batch = zip(*batch)
    
    # 填充源序列
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    
    # 填充目标序列
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建掩码
    src_mask = (src_padded != src_vocab['<pad>']).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_seq_len]
    tgt_input = tgt_padded[:, :-1]
    tgt_output = tgt_padded[:, 1:]
    tgt_mask = (tgt_padded != tgt_vocab['<pad>']).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, tgt_seq_len]
    
    return {
        'src': src_padded,
        'tgt_input': tgt_input,
        'tgt_output': tgt_output,
        'src_mask': src_mask,
        'tgt_mask': (tgt_input != tgt_vocab['<pad>'])
    }
```

### **5.3 实例化DataLoader**

```python
# 假设您有一组源语言和目标语言句子
source_sentences = [
    "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5",
    "src_word_6 src_word_7 src_word_8 src_word_9 src_word_10",
    # ... 更多句子
]

target_sentences = [
    "tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5",
    "tgt_word_6 tgt_word_7 tgt_word_8 tgt_word_9 tgt_word_10",
    # ... 更多句子
]

# 创建Dataset实例
dataset = TranslationDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)

# 创建DataLoader
from torch.nn.utils.rnn import pad_sequence

batch_size = 32

data_loader = DataLoader(
    dataset, 
    batch_size=batch_size, 
    shuffle=True, 
    collate_fn=collate_fn
)
```

---

## **6. 训练循环**

现在，我们将调整训练循环，以适应新的词汇表配置，并集成频率加权交叉熵损失。

### **6.1 定义训练函数**

```python
def train(model, data_loader, criterion, optimizer, device, epoch, clip=1.0):
    model.train()
    epoch_loss = 0
    
    for batch in data_loader:
        src = batch['src'].to(device)  # [batch_size, src_seq_len]
        tgt_input = batch['tgt_input'].to(device)  # [batch_size, tgt_seq_len -1]
        tgt_output = batch['tgt_output'].to(device)  # [batch_size, tgt_seq_len -1]
        
        optimizer.zero_grad()
        
        # 前向传播
        logits = model(src, tgt_input)  # [batch_size, tgt_seq_len -1, tgt_vocab_size]
        
        # 调整logits和targets的形状
        logits = logits.view(-1, logits.size(-1))  # [batch_size * (tgt_seq_len -1), tgt_vocab_size]
        tgt_output = tgt_output.view(-1)  # [batch_size * (tgt_seq_len -1)]
        
        # 计算损失（使用历史目标输入计算词频）
        loss = criterion(logits, tgt_output, tgt_input)  # scalar
        
        # 反向传播
        loss.backward()
        
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)
        
        # 优化器更新
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(data_loader)
    print(f"Epoch {epoch}, Training Loss: {avg_loss:.4f}")
    return avg_loss
```

### **6.2 定义验证函数**

验证函数用于评估模型在验证集上的性能。

```python
def evaluate(model, data_loader, criterion, device):
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in data_loader:
            src = batch['src'].to(device)
            tgt_input = batch['tgt_input'].to(device)
            tgt_output = batch['tgt_output'].to(device)
            
            # 前向传播
            logits = model(src, tgt_input)
            
            # 调整logits和targets的形状
            logits = logits.view(-1, logits.size(-1))
            tgt_output = tgt_output.view(-1)
            
            # 计算损失
            loss = criterion(logits, tgt_output, tgt_input)
            
            epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(data_loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    return avg_loss
```

### **6.3 实例化损失函数和优化器**

```python
# 定义损失函数
criterion = FrequencyWeightedCrossEntropyLoss(
    vocab_size=tgt_vocab_size,
    ignore_index=tgt_vocab['<pad>'],
    penalty_factor=1.0  # 根据需要调整
)

# 定义优化器
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
```

### **6.4 训练过程**

```python
num_epochs = 10
best_val_loss = float('inf')

for epoch in range(1, num_epochs + 1):
    print(f"--- Epoch {epoch} ---")
    
    # 训练
    train_loss = train(model, data_loader, criterion, optimizer, device, epoch)
    
    # 验证（假设有验证集 DataLoader, val_data_loader）
    # val_loss = evaluate(model, val_data_loader, criterion, device)
    
    # 检查是否为最佳模型，并保存
    # if val_loss < best_val_loss:
    #     best_val_loss = val_loss
    #     torch.save(model.state_dict(), 'best_model.pt')
    #     print("Saved Best Model!")
```

**注意：**
- 验证过程假设您有一个验证集的`DataLoader`，命名为`val_data_loader`。
- 保存最优模型时，注释掉的部分需根据实际情况取消注释并确保验证集存在。
- `penalty_factor`需根据实际需求和验证集表现进行调整，以找到最佳平衡点。

---

## **7. 保存和加载模型**

确保训练后模型参数得到妥善保存，以便后续加载和使用。

### **7.1 保存模型**

```python
# 保存模型参数
torch.save(model.state_dict(), 'best_model.pt')
```

### **7.2 加载模型**

```python
# 加载模型参数
model.load_state_dict(torch.load('best_model.pt', map_location=device))
model.eval()  # 设置为评估模式
```

---

## **8. 完整代码示例**

以下是整合上述步骤的完整训练过程示例。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader

# 1. 定义特殊标识符和词汇表
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39

# 2. 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 3. 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)  # [batch_size, src_seq_len, d_model]
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)  # [batch_size, tgt_seq_len, d_model]
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits

# 4. 定义频率加权交叉熵损失
class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失（忽略<pad>）
        return loss.mean()

# 5. 定义Dataset和DataLoader
class TranslationDataset(Dataset):
    def __init__(self, source_sentences, target_sentences, src_vocab, tgt_vocab):
        """
        :param source_sentences: list of str, 源语言句子
        :param target_sentences: list of str, 目标语言句子
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sentences) == len(target_sentences), "源语言和目标语言句子数量不一致"
        self.source_sentences = source_sentences
        self.target_sentences = target_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sentences)
    
    def __getitem__(self, idx):
        src_seq = tokenize_and_encode(self.source_sentences[idx], self.src_vocab)
        tgt_seq = tokenize_and_encode(self.target_sentences[idx], self.tgt_vocab)
        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)

def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标序列及对应的掩码
    """
    src_batch, tgt_batch = zip(*batch)
    
    # 填充源序列
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    
    # 填充目标序列
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建填充掩码
    src_padding_mask = (src_padded != src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_padded != tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建目标输入和输出
    tgt_input = tgt_padded[:, :-1]  # [batch_size, tgt_seq_len -1]
    tgt_output = tgt_padded[:, 1:]  # [batch_size, tgt_seq_len -1]
    
    return {
        'src': src_padded,
        'tgt_input': tgt_input,
        'tgt_output': tgt_output,
        'src_mask': src_padding_mask,
        'tgt_mask': tgt_padding_mask[:, :-1]  # 对应tgt_input
    }

# 示例句子
source_sentences = [
    "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5",
    "src_word_6 src_word_7 src_word_8 src_word_9 src_word_10",
    # ... 更多句子
]

target_sentences = [
    "tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5",
    "tgt_word_6 tgt_word_7 tgt_word_8 tgt_word_9 tgt_word_10",
    # ... 更多句子
]

# 创建Dataset
dataset = TranslationDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)

# 创建DataLoader
batch_size = 32

data_loader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_fn
)

# 假设有验证集
# val_source_sentences = [...]
# val_target_sentences = [...]
# val_dataset = TranslationDataset(val_source_sentences, val_target_sentences, src_vocab, tgt_vocab)
# val_data_loader = DataLoader(
#     val_dataset,
#     batch_size=batch_size,
#     shuffle=False,
#     collate_fn=collate_fn
# )
```

**注意：**
- 上述示例中，源语言和目标语言的句子列表仅包含两个示例句子。实际使用时，请替换为您的实际数据集。
- 验证集的部分代码已被注释，需要根据实际情况取消注释并提供验证数据。

---

## **9. 训练过程示例**

结合上述步骤，我们将展示一个完整的训练过程示例。

```python
# 设定随机种子以确保结果可复现
import random
import numpy as np

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# 实例化模型
model = TransformerModel(
    src_vocab_size=len(src_vocab),  # 64
    tgt_vocab_size=len(tgt_vocab),  # 39
    d_model=512,
    n_head=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    dim_feedforward=2048,
    dropout=0.1,
    max_len=100
)

model = model.to(device)

# 加载训练好的模型参数（如果有）
# model.load_state_dict(torch.load('best_model.pt', map_location=device))
# model.eval()

# 定义损失函数和优化器
criterion = FrequencyWeightedCrossEntropyLoss(
    vocab_size=len(tgt_vocab), 
    ignore_index=tgt_vocab['<pad>'], 
    penalty_factor=1.0
)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# 训练循环
num_epochs = 10
best_train_loss = float('inf')

for epoch in range(1, num_epochs + 1):
    print(f"--- Epoch {epoch} ---")
    
    # 训练
    train_loss = train(model, data_loader, criterion, optimizer, device, epoch)
    
    # 验证（假设有验证集 DataLoader, val_data_loader）
    # val_loss = evaluate(model, val_data_loader, criterion, device)
    
    # 检查是否为最佳模型，并保存
    # if val_loss < best_val_loss:
    #     best_val_loss = val_loss
    #     torch.save(model.state_dict(), 'best_model.pt')
    #     print("Saved Best Model!")
```

**注意：**
- 如果您有验证集，请取消相关部分注释，并确保`val_data_loader`已定义。
- `penalty_factor`可根据验证集表现调整，以平衡常见词和多样性。

---

## **10. 使用频率惩罚优化**

在频率加权损失函数中，我们已在损失计算中考虑了历史词汇频率。这种方法降低了模型对频繁词汇的依赖，促进生成多样化的翻译。

**示例：**

训练过程中，损失函数`FrequencyWeightedCrossEntropyLoss`会根据历史目标输入（前n-1个token）的词汇频率，动态调整损失权重。

确保在每个训练步骤中，`history`参数传递的是当前批次的目标输入序列（`tgt_input`），如下示例所示：

```python
# 在训练函数中调用损失函数
loss = criterion(logits, tgt_output, tgt_input)
```

这样，频率计算基于`decode_and_encode`后的`history`，即模型当前解码器的输入。

---

## **11. 完整的训练和评估示例**

以下提供一个完整的训练和评估流程示例，结合之前定义的所有组件。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
import random
import numpy as np

# 设置随机种子
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# 1. 定义特殊标识符和词汇表
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39

# 2. 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 3. 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)  # [batch_size, src_seq_len, d_model]
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)  # [batch_size, tgt_seq_len, d_model]
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits

# 4. 定义频率加权交叉熵损失
class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失（忽略<pad>）
        return loss.mean()

# 5. 定义Dataset和DataLoader
class TranslationDataset(Dataset):
    def __init__(self, source_sentences, target_sentences, src_vocab, tgt_vocab):
        """
        :param source_sentences: list of str, 源语言句子
        :param target_sentences: list of str, 目标语言句子
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sentences) == len(target_sentences), "源语言和目标语言句子数量不一致"
        self.source_sentences = source_sentences
        self.target_sentences = target_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sentences)
    
    def __getitem__(self, idx):
        src_seq = tokenize_and_encode(self.source_sentences[idx], self.src_vocab)
        tgt_seq = tokenize_and_encode(self.target_sentences[idx], self.tgt_vocab)
        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)

def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标序列及对应的掩码
    """
    src_batch, tgt_batch = zip(*batch)
    
    # 填充源序列
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    
    # 填充目标序列
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建填充掩码
    src_padding_mask = (src_padded != src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_padded != tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建目标输入和输出
    tgt_input = tgt_padded[:, :-1]  # [batch_size, tgt_seq_len -1]
    tgt_output = tgt_padded[:, 1:]  # [batch_size, tgt_seq_len -1]
    
    return {
        'src': src_padded,
        'tgt_input': tgt_input,
        'tgt_output': tgt_output,
        'src_mask': src_padding_mask,
        'tgt_mask': tgt_padding_mask[:, :-1]  # 对应tgt_input
    }

# 示例句子
source_sentences = [
    "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5",
    "src_word_6 src_word_7 src_word_8 src_word_9 src_word_10",
    # ... 更多句子
]

target_sentences = [
    "tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5",
    "tgt_word_6 tgt_word_7 tgt_word_8 tgt_word_9 tgt_word_10",
    # ... 更多句子
]

# 创建Dataset
dataset = TranslationDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)

# 创建DataLoader
batch_size = 32

data_loader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_fn
)

# 如果有验证集，可以定义验证集的DataLoader
# val_source_sentences = [...]
# val_target_sentences = [...]
# val_dataset = TranslationDataset(val_source_sentences, val_target_sentences, src_vocab, tgt_vocab)
# val_data_loader = DataLoader(
#     val_dataset,
#     batch_size=batch_size,
#     shuffle=False,
#     collate_fn=collate_fn
# )
```

---

## **12. 完整训练和评估流程**

结合之前定义的所有组件，以下是一个完整的训练和评估流程示例。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
import random
import numpy as np

# 设置随机种子以确保结果可复现
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# 1. 定义特殊标识符和词汇表
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39

# 2. 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 3. 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)  # [batch_size, src_seq_len, d_model]
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)  # [batch_size, tgt_seq_len, d_model]
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits

# 4. 定义频率加权交叉熵损失
class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失（忽略<pad>）
        return loss.mean()

# 5. 定义Dataset和DataLoader
class TranslationDataset(Dataset):
    def __init__(self, source_sentences, target_sentences, src_vocab, tgt_vocab):
        """
        :param source_sentences: list of str, 源语言句子
        :param target_sentences: list of str, 目标语言句子
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sentences) == len(target_sentences), "源语言和目标语言句子数量不一致"
        self.source_sentences = source_sentences
        self.target_sentences = target_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sentences)
    
    def __getitem__(self, idx):
        src_seq = tokenize_and_encode(self.source_sentences[idx], self.src_vocab)
        tgt_seq = tokenize_and_encode(self.target_sentences[idx], self.tgt_vocab)
        return torch.tensor(src_seq, dtype=torch.long), torch.tensor(tgt_seq, dtype=torch.long)

def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标序列及对应的掩码
    """
    src_batch, tgt_batch = zip(*batch)
    
    # 填充源序列
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    
    # 填充目标序列
    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建填充掩码
    src_padding_mask = (src_padded != src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_padded != tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建目标输入和输出
    tgt_input = tgt_padded[:, :-1]  # [batch_size, tgt_seq_len -1]
    tgt_output = tgt_padded[:, 1:]  # [batch_size, tgt_seq_len -1]
    
    return {
        'src': src_padded,
        'tgt_input': tgt_input,
        'tgt_output': tgt_output,
        'src_mask': src_padding_mask,
        'tgt_mask': tgt_padding_mask[:, :-1]  # 对应tgt_input
    }

# 示例句子（需替换为实际数据）
source_sentences = [
    "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5",
    "src_word_6 src_word_7 src_word_8 src_word_9 src_word_10",
    # ... 更多句子
]

target_sentences = [
    "tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5",
    "tgt_word_6 tgt_word_7 tgt_word_8 tgt_word_9 tgt_word_10",
    # ... 更多句子
]

# 创建Dataset
dataset = TranslationDataset(source_sentences, target_sentences, src_vocab, tgt_vocab)

# 创建DataLoader
batch_size = 32

data_loader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_fn
)

# 如果有验证集，可以定义验证集的DataLoader
# val_source_sentences = [...]
# val_target_sentences = [...]
# val_dataset = TranslationDataset(val_source_sentences, val_target_sentences, src_vocab, tgt_vocab)
# val_data_loader = DataLoader(
#     val_dataset,
#     batch_size=batch_size,
#     shuffle=False,
#     collate_fn=collate_fn
# )

# 6. 定义训练和评估函数
def train(model, data_loader, criterion, optimizer, device, epoch, clip=1.0):
    model.train()
    epoch_loss = 0
    
    for batch in data_loader:
        src = batch['src'].to(device)  # [batch_size, src_seq_len]
        tgt_input = batch['tgt_input'].to(device)  # [batch_size, tgt_seq_len -1]
        tgt_output = batch['tgt_output'].to(device)  # [batch_size, tgt_seq_len -1]
        
        optimizer.zero_grad()
        
        # 前向传播
        logits = model(src, tgt_input)  # [batch_size, tgt_seq_len -1, tgt_vocab_size]
        
        # 调整logits和targets的形状
        logits = logits.view(-1, logits.size(-1))  # [batch_size * (tgt_seq_len -1), tgt_vocab_size]
        tgt_output = tgt_output.view(-1)  # [batch_size * (tgt_seq_len -1)]
        
        # 计算损失（使用历史目标输入计算词频）
        loss = criterion(logits, tgt_output, tgt_input)  # scalar
        
        # 反向传播
        loss.backward()
        
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)
        
        # 优化器更新
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(data_loader)
    print(f"Epoch {epoch}, Training Loss: {avg_loss:.4f}")
    return avg_loss

def evaluate(model, data_loader, criterion, device):
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in data_loader:
            src = batch['src'].to(device)
            tgt_input = batch['tgt_input'].to(device)
            tgt_output = batch['tgt_output'].to(device)
            
            # 前向传播
            logits = model(src, tgt_input)
            
            # 调整logits和targets的形状
            logits = logits.view(-1, logits.size(-1))
            tgt_output = tgt_output.view(-1)
            
            # 计算损失
            loss = criterion(logits, tgt_output, tgt_input)
            
            epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(data_loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    return avg_loss

# 7. 实例化模型、损失函数和优化器
model = TransformerModel(
    src_vocab_size=len(src_vocab),  # 64
    tgt_vocab_size=len(tgt_vocab),  # 39
    d_model=512,
    n_head=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    dim_feedforward=2048,
    dropout=0.1,
    max_len=100
)

model = model.to(device)

# 定义损失函数
criterion = FrequencyWeightedCrossEntropyLoss(
    vocab_size=len(tgt_vocab), 
    ignore_index=tgt_vocab['<pad>'], 
    penalty_factor=1.0  # 根据需要调整
)

# 定义优化器
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# 8. 训练过程中保存最优模型
num_epochs = 10
best_val_loss = float('inf')

for epoch in range(1, num_epochs + 1):
    print(f"--- Epoch {epoch} ---")
    
    # 训练
    train_loss = train(model, data_loader, criterion, optimizer, device, epoch)
    
    # 验证（如果有验证集）
    # val_loss = evaluate(model, val_data_loader, criterion, device)
    
    # 检查是否为最佳模型，并保存
    # if val_loss < best_val_loss:
    #     best_val_loss = val_loss
    #     torch.save(model.state_dict(), 'best_model.pt')
    #     print("Saved Best Model!")
```

**注意：**
- **验证集**：如果有验证集，请取消相关部分注释，并确保`val_data_loader`已正确定义。
- **频率惩罚力度**：根据验证集表现灵活调整`penalty_factor`。
- **模型保存**：确保在训练结束后保存模型，以便后续加载和使用。

---

## **13. 使用训练好的模型进行预测**

训练完成后，您可以使用训练好的模型对新的源语言句子进行翻译。以下是预测过程的详细步骤。

### **13.1 定义预测函数**

```python
def greedy_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用贪婪搜索进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of str, 翻译后的目标语言句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (torch.tensor(tgt_indices).unsqueeze(0) != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 选择最大概率的词
            _, next_token = torch.max(next_token_logits, dim=-1)  # [1]
            next_token = next_token.item()
            
            # 如果生成结束符，停止
            if next_token == tgt_vocab['<eos>']:
                break
            
            # 添加到目标序列
            tgt_indices.append(next_token)
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence
```

### **13.2 定义束搜索解码函数**

束搜索提供了比贪婪搜索更好的翻译质量，适用于需要高质量翻译的场景。

```python
class BeamSearchNode:
    def __init__(self, previous_node, word_id, log_prob, length):
        self.previous_node = previous_node
        self.word_id = word_id
        self.log_prob = log_prob
        self.length = length
    
    def __lt__(self, other):
        return self.log_prob < other.log_prob

def beam_search_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, beam_width=3, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用束搜索进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param beam_width: int, 束宽度
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of str, 最佳的翻译句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [batch_size, src_seq_len]
        )
    
    # 初始化束
    BOS = tgt_vocab['<sos>']
    EOS = tgt_vocab['<eos>']
    candidate_nodes = []
    root = BeamSearchNode(previous_node=None, word_id=BOS, log_prob=0.0, length=1)
    candidate_nodes.append((-root.log_prob, root))  # 使用负的log_prob作为最小堆
    
    # 初始化结束节点
    end_nodes = []
    
    for _ in range(max_len):
        next_candidate_nodes = []
        current_beam_size = len(candidate_nodes)
        for _ in range(current_beam_size):
            score, node = heapq.heappop(candidate_nodes)
            if node.word_id == EOS:
                end_nodes.append((score, node))
                continue
            # 生成当前节点的扩展
            tgt_indices = []
            current = node
            while current is not None:
                tgt_indices.append(current.word_id)
                current = current.previous_node
            tgt_indices = tgt_indices[::-1]
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
            
            # 创建目标掩码
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
            
            # 创建目标填充掩码
            tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
            
            with torch.no_grad():
                output = model.transformer.decoder(
                    model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                    memory, 
                    tgt_mask=tgt_mask, 
                    tgt_key_padding_mask=tgt_padding_mask, 
                    memory_key_padding_mask=src_padding_mask
                )  # [tgt_seq_len, 1, d_model]
                
                # 通过线性层获得logits
                logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
                
                # 获取最后一个时间步的logit
                next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
                
                # 计算对数概率
                log_probs = F.log_softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 选择前beam_width个可能的词
            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)
            
            for k in range(beam_width):
                next_word_id = topk_indices[k].item()
                next_log_prob = node.log_prob + topk_log_probs[k].item()
                next_node = BeamSearchNode(
                    previous_node=node,
                    word_id=next_word_id,
                    log_prob=next_log_prob,
                    length=node.length + 1
                )
                heapq.heappush(next_candidate_nodes, (-next_node.log_prob, next_node))
        
        # 保留当前束宽度的最佳节点
        for _ in range(min(beam_width, len(next_candidate_nodes))):
            heapq.heappush(candidate_nodes, heapq.heappop(next_candidate_nodes))
        
        # 如果所有候选节点都生成了<eos>，停止
        if len(candidate_nodes) == 0:
            break
    
    # 添加剩余的节点到结束节点
    while len(candidate_nodes) > 0:
        score, node = heapq.heappop(candidate_nodes)
        if node.word_id == EOS:
            end_nodes.append((score, node))
    
    # 如果没有结束节点，则选择束中的最佳节点
    if len(end_nodes) == 0:
        end_nodes = candidate_nodes
    
    # 选择最优节点
    end_nodes = sorted(end_nodes, key=lambda x: x[0])
    best_node = end_nodes[0][1]
    
    # 回溯生成序列
    tgt_indices = []
    current = best_node
    while current is not None:
        tgt_indices.append(current.word_id)
        current = current.previous_node
    tgt_indices = tgt_indices[::-1]  # 反转
    
    # 移除<sos>并终止于<eos>
    if tgt_indices[0] == BOS:
        tgt_indices = tgt_indices[1:]
    if EOS in tgt_indices:
        tgt_indices = tgt_indices[:tgt_indices.index(EOS)]
    
    # 转换为词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices]
    return predicted_sentence
```

### **13.3 示例预测**

```python
# 示例句子
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"

# 编码
source_ids = tokenize_and_encode(source_sentence, src_vocab)  # [1, 4, 5, 6, 7, 2]

# 贪婪搜索预测
translation_greedy = greedy_decode(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    max_len=5, 
    device=device
)
print("Greedy Search Translation:", ' '.join(translation_greedy))

# 束搜索预测
translation_beam = beam_search_decode(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    beam_width=3, 
    max_len=5, 
    device=device
)
print("Beam Search Translation:", ' '.join(translation_beam))

# 使用频率惩罚的贪婪搜索预测
def greedy_decode_with_frequency_penalty(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu', penalty_factor=1.0):
    """
    使用贪婪搜索并结合频率惩罚进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :param penalty_factor: float, 频率惩罚力度
    :return: list of str, 翻译后的目标语言句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    current_frequency = {}  # 用于记录历史词频
    
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 应用频率惩罚
            penalty = torch.zeros_like(next_token_logits).to(device)
            for word_id, count in current_frequency.items():
                penalty[word_id] = penalty_factor * count
            adjusted_logits = next_token_logits - penalty  # 降低频繁词的logit值
            
            # 计算概率
            probs = F.softmax(adjusted_logits, dim=-1)  # [tgt_vocab_size]
        
        # 选择最大概率的词
        _, next_token = torch.max(probs, dim=-1)  # [1]
        next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
        
        # 更新当前频率
        if next_token in current_frequency:
            current_frequency[next_token] += 1
        else:
            current_frequency[next_token] = 1
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence

translation_penalty = greedy_decode_with_frequency_penalty(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    max_len=5, 
    device=device, 
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", ' '.join(translation_penalty))
```

**输出示例：**
```
Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Beam Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Frequency Penalty Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
```

**注意：**
- 以上输出仅为示例。实际输出取决于您的训练数据和模型性能。
- 若生成过程中遇到 `<eos>`，则提前停止生成。

---

## **14. 结论**

通过上述步骤，我们成功调整了训练过程以适应新的词汇表配置（源语言60词 + 4特殊标识符，目标语言35词 + 4特殊标识符）。以下是关键步骤的总结：

1. **词汇表构建**：
    - 为源语言和目标语言分别构建包含特殊标识符的词汇表。
  
2. **数据预处理**：
    - 分词，添加特殊标识符，编码，并填充序列以适应批量训练。
  
3. **模型定义调整**：
    - 确保嵌入层和输出层的词汇表大小与新的配置一致。
  
4. **损失函数设计**：
    - 采用频率加权交叉熵损失，降低历史中频繁词汇的损失权重，促进生成多样性。

5. **训练循环**：
    - 定义训练和验证函数，执行前向传播、损失计算、反向传播与参数更新。
  
6. **模型保存和加载**：
    - 在训练过程中保存最优模型，确保最佳性能的模型得到保存和后续使用。
  
7. **预测过程**：
    - 定义并实现贪婪搜索和束搜索解码策略，用于生成翻译结果。
    - 集成频率惩罚机制，进一步优化生成的翻译质量和多样性。

通过系统地调整这些组件，您能够成功训练并使用自定义配置的Transformer模型进行机器翻译任务。根据实际需求和数据集特点，进一步优化模型结构、损失函数和解码策略，可以显著提升翻译质量和模型性能。

如您在实施过程中遇到任何问题或需要进一步的澄清与辅助，欢迎继续提问！