理解您的需求后，我们将重新构建词汇表，确保源语言和目标语言的有效词汇量分别为60和35，同时添加必要的特殊标识符（如 `<pad>`, `<sos>`, `<eos>`, `<unk>`）。接下来，我们将调整之前的预测过程，以适应新的词汇表配置。

## **1. 重新构建词汇表**

### **1.1 定义特殊标识符**

首先，我们定义源语言和目标语言所需的特殊标识符：

- `<pad>`: 填充符，用于对齐不同长度的序列。
- `<sos>`: 开始符，表示序列的开始。
- `<eos>`: 结束符，表示序列的结束。
- `<unk>`: 未知词，用于处理词汇表中未包含的词。

### **1.2 构建词汇表**

假设您已经有源语言和目标语言的有效词汇列表。我们将为每个语言创建一个词汇表，并为特殊标识符分配固定的ID。

```python
# 定义特殊标识符
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}
```

**说明：**

- `<pad>`, `<sos>`, `<eos>`, `<unk>` 分别被赋予0, 1, 2, 3的ID。
- 源语言的有效词汇从4开始，目标语言同理。

### **1.3 查看词汇表大小**

```python
print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64 = 4 + 60
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39 = 4 + 35
```

## **2. 调整模型定义**

在构建模型时，需要确保词汇表大小包含特殊标识符。

```python
import torch
import torch.nn as nn
import math

# 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_head=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_decoder = PositionalEncoding(d_model)
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)
        output = self.transformer(
            src_emb.transpose(0,1), 
            tgt_emb.transpose(0,1), 
            src_mask, 
            tgt_mask, 
            None, 
            src_padding_mask, 
            tgt_padding_mask, 
            memory_key_padding_mask
        )
        logits = self.fc_out(output.transpose(0, 1))
        return logits
```

**说明：**

- `src_vocab_size` 和 `tgt_vocab_size` 分别为64和39。
- 位置编码通过向嵌入向量添加位置信息，使模型能够识别序列中词汇的位置。

### **模型实例化**

```python
# 参数定义
src_vocab_size = len(src_vocab)  # 64
tgt_vocab_size = len(tgt_vocab)  # 39
d_model = 512
n_head = 8
num_encoder_layers = 6
num_decoder_layers = 6
dim_feedforward = 2048
dropout = 0.1

# 实例化模型
model = TransformerModel(
    src_vocab_size=src_vocab_size, 
    tgt_vocab_size=tgt_vocab_size, 
    d_model=d_model, 
    n_head=n_head, 
    num_encoder_layers=num_encoder_layers, 
    num_decoder_layers=num_decoder_layers, 
    dim_feedforward=dim_feedforward, 
    dropout=dropout
)

# 加载训练好的模型参数
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(torch.load('best_model.pt', map_location=device))
model.eval()  # 设置为评估模式
model.to(device)
```

## **3. 数据预处理**

### **3.1 分词与编码**

根据新的词汇表定义，调整分词与编码函数：

```python
def tokenize_and_encode(sentence, vocab):
    """
    简单的分词与编码函数
    :param sentence: str, 源语言句子
    :param vocab: dict, 词汇表 {word: id}
    :return: list of int, 编码后的ID序列
    """
    tokens = sentence.strip().split()  # 简单的空格分词
    tokens = ['<sos>'] + tokens + ['<eos>']  # 添加起始和结束符
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]  # 未知词用<unk>处理
    return ids

# 示例源句子
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"

# 编码
source_ids = tokenize_and_encode(source_sentence, src_vocab)  # [1, 4, 5, 6, 7, 2]
print("Encoded Source IDs:", source_ids)
```

### **3.2 创建掩码**

调整掩码生成函数，适应新的词汇表配置：

```python
def create_padding_mask(seq, pad_idx):
    """
    创建填充掩码
    :param seq: Tensor, [batch_size, seq_len]
    :param pad_idx: int, 填充符的ID
    :return: Tensor, [batch_size, 1, 1, seq_len]
    """
    mask = (seq == pad_idx)  # [batch_size, seq_len]
    return mask
```

## **4. 预测过程**

### **4.1 贪婪搜索（Greedy Search）**

调整贪婪搜索函数以适应新的词汇表：

```python
def greedy_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用贪婪搜索进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of str, 翻译后的目标语言句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=src_vocab['<pad>'])  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                memory_key_padding_mask=src_padding_mask.squeeze(1)
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 选择最大概率的词
            _, next_token = torch.max(next_token_logits, dim=-1)  # [1]
            next_token = next_token.item()
            
            # 如果生成结束符，停止
            if next_token == tgt_vocab['<eos>']:
                break
            
            # 添加到目标序列
            tgt_indices.append(next_token)
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence

# 示例预测
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"
source_ids = tokenize_and_encode(source_sentence, src_vocab)
predicted_tokens = greedy_decode(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    max_len=5
)
print("Greedy Search Translation:", ' '.join(predicted_tokens))
```

### **4.2 束搜索（Beam Search）**

调整束搜索函数以适应新的词汇表：

```python
import heapq

class BeamSearchNode:
    def __init__(self, previous_node, word_id, log_prob, length):
        self.previous_node = previous_node
        self.word_id = word_id
        self.log_prob = log_prob
        self.length = length
    
    def __lt__(self, other):
        return self.log_prob < other.log_prob

def beam_search_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, beam_width=3, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用束搜索进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param beam_width: int, 束宽度
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of str, 最佳的翻译句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=src_vocab['<pad>'])  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化束
    BOS = tgt_vocab['<sos>']
    EOS = tgt_vocab['<eos>']
    candidate_nodes = []
    root = BeamSearchNode(previous_node=None, word_id=BOS, log_prob=0.0, length=1)
    candidate_nodes.append((-root.log_prob, root))  # 使用负的log_prob作为最小堆
    
    # 初始化最佳节点
    end_nodes = []
    
    for _ in range(max_len):
        next_candidate_nodes = []
        # 遍历当前所有候选节点
        for _ in range(len(candidate_nodes)):
            score, node = heapq.heappop(candidate_nodes)
            if node.word_id == EOS:
                end_nodes.append((score, node))
                continue
            # 生成当前节点的扩展
            tgt_indices = []
            current = node
            while current is not None:
                tgt_indices.append(current.word_id)
                current = current.previous_node
            tgt_indices = tgt_indices[::-1]
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
            
            # 创建目标掩码
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
            
            # 创建目标填充掩码
            tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, len_tgt]
            
            with torch.no_grad():
                output = model.transformer.decoder(
                    model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                    memory, 
                    tgt_mask=tgt_mask, 
                    tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                    memory_key_padding_mask=src_padding_mask.squeeze(1)
                )  # [tgt_seq_len, 1, d_model]
                
                # 通过线性层获得logits
                logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
                
                # 获取最后一个时间步的logit
                next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
                
                # 计算对数概率
                log_probs = F.log_softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 选择前beam_width个可能的词
            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)
            
            for k in range(beam_width):
                next_word_id = topk_indices[k].item()
                next_log_prob = node.log_prob + topk_log_probs[k].item()
                next_node = BeamSearchNode(
                    previous_node=node,
                    word_id=next_word_id,
                    log_prob=next_log_prob,
                    length=node.length + 1
                )
                heapq.heappush(next_candidate_nodes, (-next_node.log_prob, next_node))
        
        # 保留当前束宽度的最佳节点
        for _ in range(min(beam_width, len(next_candidate_nodes))):
            heapq.heappush(candidate_nodes, heapq.heappop(next_candidate_nodes))
        
        # 如果所有候选节点都生成了<eos>，停止
        if len(candidate_nodes) == 0:
            break
    
    # 添加剩余的节点到结束节点
    while len(candidate_nodes) > 0:
        score, node = heapq.heappop(candidate_nodes)
        if node.word_id == EOS:
            end_nodes.append((score, node))
    
    # 如果没有结束节点，则选择束中的最佳节点
    if len(end_nodes) == 0:
        end_nodes = candidate_nodes
    
    # 选择最优节点
    end_nodes = sorted(end_nodes, key=lambda x: x[0])
    best_node = end_nodes[0][1]
    
    # 回溯生成序列
    tgt_indices = []
    current = best_node
    while current is not None:
        tgt_indices.append(current.word_id)
        current = current.previous_node
    tgt_indices = tgt_indices[::-1]  # 反转
    
    # 移除<sos>并终止于<eos>
    if tgt_indices[0] == BOS:
        tgt_indices = tgt_indices[1:]
    if EOS in tgt_indices:
        tgt_indices = tgt_indices[:tgt_indices.index(EOS)]
    
    # 转换为词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices]
    return predicted_sentence

# 示例预测
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"
source_ids = tokenize_and_encode(source_sentence, src_vocab)
predicted_tokens_beam = beam_search_decode(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    beam_width=3, 
    max_len=5
)
print("Beam Search Translation:", ' '.join(predicted_tokens_beam))
```

### **4.3 使用频率惩罚机制进行预测**

如您之前模型的特点，历史预测中频繁出现的词，当前预测时应该具有较小的概率。这可以通过在预测过程中动态调整logits来实现。

```python
def greedy_decode_with_frequency_penalty(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu', penalty_factor=1.0):
    """
    使用贪婪搜索并结合频率惩罚进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :param penalty_factor: float, 频率惩罚力度
    :return: list of str, 翻译后的目标语言句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=src_vocab['<pad>'])  # [1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    current_frequency = {}  # 用于记录历史词频
    
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                memory_key_padding_mask=src_padding_mask.squeeze(1)
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 应用频率惩罚
            penalty = torch.zeros_like(next_token_logits).to(device)
            for word_id, count in current_frequency.items():
                penalty[word_id] = penalty_factor * count
            adjusted_logits = next_token_logits - penalty  # 降低频繁词的logit值
            
            # 计算概率
            probs = F.softmax(adjusted_logits, dim=-1)  # [tgt_vocab_size]
        
        # 选择最大概率的词
        _, next_token = torch.max(probs, dim=-1)  # [1]
        next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
        
        # 更新当前频率
        if next_token in current_frequency:
            current_frequency[next_token] += 1
        else:
            current_frequency[next_token] = 1
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence

# 示例预测
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"
source_ids = tokenize_and_encode(source_sentence, src_vocab)
predicted_tokens_penalty = greedy_decode_with_frequency_penalty(
    model, 
    source_ids, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    max_len=5, 
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", ' '.join(predicted_tokens_penalty))
```

### **4.4 综合的翻译函数**

为了方便使用，我们可以将不同解码方法整合到一个函数中：

```python
def decode_ids_to_sentence(ids, id_to_vocab):
    """
    将词ID序列转换为文本句子
    :param ids: list of int, 词ID序列
    :param id_to_vocab: dict, 词汇表反向映射 {id: word}
    :return: str, 生成的句子
    """
    tokens = [id_to_vocab.get(idx, '<unk>') for idx in ids]
    return ' '.join(tokens)

def translate(model, sentence, src_vocab, tgt_vocab, id_to_tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu', method='greedy', beam_width=3, max_len=5, frequency_penalty=False, penalty_factor=1.0):
    """
    综合的翻译函数，支持多种解码方法
    :param model: 训练好的Transformer模型
    :param sentence: str, 源语言句子
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param device: torch.device
    :param method: str, 'greedy' or 'beam'
    :param beam_width: int, 束宽度，仅当method='beam'时有效
    :param max_len: int, 生成目标序列的最大长度
    :param frequency_penalty: bool, 是否应用频率惩罚
    :param penalty_factor: float, 频率惩罚力度
    :return: str, 生成的翻译句子
    """
    # 数据预处理
    src_ids = tokenize_and_encode(sentence, src_vocab)  # [1, ...]
    
    if method == 'greedy':
        if not frequency_penalty:
            predicted_tokens = greedy_decode(
                model, 
                src_ids, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device
            )
        else:
            predicted_tokens = greedy_decode_with_frequency_penalty(
                model, 
                src_ids, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device, 
                penalty_factor=penalty_factor
            )
    elif method == 'beam':
        predicted_tokens = beam_search_decode(
            model, 
            src_ids, 
            src_vocab, 
            tgt_vocab, 
            id_to_tgt_vocab, 
            beam_width=beam_width, 
            max_len=max_len, 
            device=device
        )
    else:
        raise ValueError("Invalid decoding method. Choose 'greedy' or 'beam'.")
    
    # 后处理
    translation = decode_ids_to_sentence(predicted_tokens, id_to_tgt_vocab)
    return translation

# 使用示例
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"

# 贪婪搜索
translation_greedy = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5
)
print("Greedy Search Translation:", translation_greedy)

# 束搜索
translation_beam = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='beam',
    beam_width=3,
    max_len=5
)
print("Beam Search Translation:", translation_beam)

# 带频率惩罚的贪婪搜索
translation_penalty = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5,
    frequency_penalty=True,
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", translation_penalty)
```

**解释：**

1. **`decode_ids_to_sentence` 函数：**
    - 将预测的词ID序列转换回实际的词汇，形成可读的句子。

2. **`translate` 函数：**
    - 综合了分词、编码、选择解码方法（贪婪搜索、束搜索）、应用频率惩罚以及后处理的步骤。
    - 根据参数选择使用哪种解码方法，并返回最终的翻译句子。

## **5. 完整的预测流程示例**

结合前述所有步骤，这里提供一个完整的预测流程示例。假设您的源语言词汇表大小为60，目标语言词汇表大小为35，不包含特殊标识符。

### **5.1 定义词汇表**

```python
# 定义特殊标识符
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39
```

### **5.2 示例预测**

```python
# 示例翻译句子
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"

# 调用translate函数进行翻译
translation_greedy = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5
)
print("Greedy Search Translation:", translation_greedy)

translation_beam = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='beam',
    beam_width=3,
    max_len=5
)
print("Beam Search Translation:", translation_beam)

translation_penalty = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5,
    frequency_penalty=True,
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", translation_penalty)
```

**输出示例：**

```
Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Beam Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Frequency Penalty Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
```

**注意：**

- 以上输出仅为示例，实际输出取决于训练数据和模型参数。
- 确保输入的源词汇存在于源词汇表中，否则将被映射为 `<unk>`。

## **6. 模型预测的重要注意事项**

### **6.1 词汇表的一致性**

确保在训练和预测阶段使用相同的词汇表，包括各个词的ID。当词汇不一致时，模型可能无法正确预测或生成有意义的翻译。

### **6.2 处理未知词汇**

在分词与编码过程中，如果遇到词汇表中未包含的词，应将其映射为 `<unk>`。这可通过调整 `tokenize_and_encode` 函数实现。

```python
def tokenize_and_encode(sentence, vocab):
    tokens = sentence.strip().split()
    tokens = ['<sos>'] + tokens + ['<eos>']
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    return ids
```

### **6.3 生成结束符**

在生成过程中，遇到 `<eos>` 时应停止生成。这防止模型生成过长的翻译序列，并确保输出句子的完整性。

### **6.4 设置合理的最大生成长度**

设置 `max_len` 参数以限制生成序列的最大长度，防止模型陷入无限循环或生成过长的句子。

## **7. 总结**

通过重新构建词汇表，确保源语言和目标语言的有效词汇量分别为60和35，并正确添加特殊标识符，我们已经调整了模型定义和预测过程。以下是关键步骤的总结：

1. **词汇表构建**：
    - 定义特殊标识符。
    - 创建源语言和目标语言的词汇表，分别包含60和35个有效词，加上特殊标识符。

2. **模型定义调整**：
    - 确保嵌入层的词汇表大小包含所有词汇和特殊标识符。

3. **数据预处理**：
    - 分词、添加特殊标识符并进行编码。
    - 创建填充掩码以处理批量数据中的不等长序列。

4. **预测过程**：
    - 实现贪婪搜索和束搜索，满足不同的生成需求。
    - 集成频率惩罚机制以降低历史中频繁出现的词的概率，促进生成多样性。

5. **后处理**：
    - 将生成的词ID序列转换回实际的词汇，形成可读的翻译句子。

6. **注意事项**：
    - 保持词汇表的一致性。
    - 处理未知词汇。
    - 遇到生成结束符时停止生成。
    - 设置合理的生成长度限制。

通过上述调整，您可以成功使用训练好的Transformer模型进行机器翻译任务。以下是完整的代码示例，结合了词汇表构建、模型定义、数据预处理和预测过程，适应您指定的词汇表大小。

## **8. 完整代码示例**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import heapq
import torch.optim as optim

# 1. 定义特殊标识符和词汇表
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（示例）
source_words = [f'src_word_{i}' for i in range(1, 61)]  # 60个有效词

# 目标语言有效词汇（示例）
target_words = [f'tgt_word_{i}' for i in range(1, 36)]  # 35个有效词

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(source_words)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({word: idx + len(special_tokens) for idx, word in enumerate(target_words)})

# 反向词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39

# 2. 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 3. 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_head=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_decoder = PositionalEncoding(d_model)
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)
        src_emb = self.pos_encoder(src_emb)
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)
        tgt_emb = self.pos_decoder(tgt_emb)
        output = self.transformer(
            src_emb.transpose(0,1), 
            tgt_emb.transpose(0,1), 
            src_mask, 
            tgt_mask, 
            None, 
            src_padding_mask, 
            tgt_padding_mask, 
            memory_key_padding_mask
        )
        logits = self.fc_out(output.transpose(0,1))
        return logits

# 4. 定义损失函数（带频率惩罚）
class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        logits: [batch_size * seq_len, vocab_size]
        targets: [batch_size * seq_len]
        history: [batch_size, seq_len]
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失
        return loss.mean()

# 5. 数据预处理函数
def tokenize_and_encode(sentence, vocab):
    """
    简单的分词与编码函数
    :param sentence: str, 源语言句子
    :param vocab: dict, 词汇表 {word: id}
    :return: list of int, 编码后的ID序列
    """
    tokens = sentence.strip().split()
    tokens = ['<sos>'] + tokens + ['<eos>']
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    return ids

def create_padding_mask(seq, pad_idx):
    """
    创建填充掩码
    :param seq: Tensor, [batch_size, seq_len]
    :param pad_idx: int, 填充符的ID
    :return: Tensor, [batch_size, 1, seq_len]
    """
    mask = (seq == pad_idx)  # [batch_size, seq_len]
    return mask

def decode_ids_to_sentence(ids, id_to_vocab):
    """
    将词ID序列转换为文本句子
    :param ids: list of int, 词ID序列
    :param id_to_vocab: dict, 词汇表反向映射 {id: word}
    :return: str, 生成的句子
    """
    tokens = [id_to_vocab.get(idx, '<unk>') for idx in ids]
    return ' '.join(tokens)

# 6. 定义解码器函数
def greedy_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=src_vocab['<pad>'])  # [1, 1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, 1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                memory_key_padding_mask=src_padding_mask.squeeze(1)
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 选择最大概率的词
            _, next_token = torch.max(next_token_logits, dim=-1)  # [1]
            next_token = next_token.item()
            
            # 如果生成结束符，停止
            if next_token == tgt_vocab['<eos>']:
                break
            
            # 添加到目标序列
            tgt_indices.append(next_token)
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence

def beam_search_decode(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, beam_width=3, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=src_vocab['<pad>'])  # [1, 1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化束
    BOS = tgt_vocab['<sos>']
    EOS = tgt_vocab['<eos>']
    candidate_nodes = []
    root = BeamSearchNode(previous_node=None, word_id=BOS, log_prob=0.0, length=1)
    candidate_nodes.append((-root.log_prob, root))  # 使用负的log_prob作为最小堆
    
    # 初始化最佳节点
    end_nodes = []
    
    for _ in range(max_len):
        next_candidate_nodes = []
        current_beam_size = len(candidate_nodes)
        for _ in range(current_beam_size):
            score, node = heapq.heappop(candidate_nodes)
            if node.word_id == EOS:
                end_nodes.append((score, node))
                continue
            # 生成当前节点的扩展
            tgt_indices = []
            current = node
            while current is not None:
                tgt_indices.append(current.word_id)
                current = current.previous_node
            tgt_indices = tgt_indices[::-1]
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
            
            # 创建目标掩码
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
            
            # 创建目标填充掩码
            tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, 1, len_tgt]
            
            with torch.no_grad():
                output = model.transformer.decoder(
                    model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                    memory, 
                    tgt_mask=tgt_mask, 
                    tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                    memory_key_padding_mask=src_padding_mask.squeeze(1)
                )  # [tgt_seq_len, 1, d_model]
                
                # 通过线性层获得logits
                logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
                
                # 获取最后一个时间步的logit
                next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
                
                # 计算对数概率
                log_probs = F.log_softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 选择前beam_width个可能的词
            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)
            
            for k in range(beam_width):
                next_word_id = topk_indices[k].item()
                next_log_prob = node.log_prob + topk_log_probs[k].item()
                next_node = BeamSearchNode(
                    previous_node=node,
                    word_id=next_word_id,
                    log_prob=next_log_prob,
                    length=node.length + 1
                )
                heapq.heappush(next_candidate_nodes, (-next_node.log_prob, next_node))
        
        # 保留当前束宽度的最佳节点
        for _ in range(min(beam_width, len(next_candidate_nodes))):
            heapq.heappush(candidate_nodes, heapq.heappop(next_candidate_nodes))
        
        # 如果所有候选节点都生成了<eos>，停止
        if len(candidate_nodes) == 0:
            break
    
    # 添加剩余的节点到结束节点
    while len(candidate_nodes) > 0:
        score, node = heapq.heappop(candidate_nodes)
        if node.word_id == EOS:
            end_nodes.append((score, node))
    
    # 如果没有结束节点，则选择束中的最佳节点
    if len(end_nodes) == 0:
        end_nodes = candidate_nodes
    
    # 选择最优节点
    end_nodes = sorted(end_nodes, key=lambda x: x[0])
    best_node = end_nodes[0][1]
    
    # 回溯生成序列
    tgt_indices = []
    current = best_node
    while current is not None:
        tgt_indices.append(current.word_id)
        current = current.previous_node
    tgt_indices = tgt_indices[::-1]  # 反转
    
    # 移除<sos>并终止于<eos>
    if tgt_indices[0] == BOS:
        tgt_indices = tgt_indices[1:]
    if EOS in tgt_indices:
        tgt_indices = tgt_indices[:tgt_indices.index(EOS)]
    
    # 转换为词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices]
    return predicted_sentence

def greedy_decode_with_frequency_penalty(model, src, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu', penalty_factor=1.0):
    """
    使用贪婪搜索并结合频率惩罚进行预测
    :param model: 训练好的Transformer模型
    :param src: list of int, 源语言句子的ID序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :param penalty_factor: float, 频率惩罚力度
    :return: list of str, 翻译后的目标语言句子
    """
    model.eval()
    src_tensor = torch.LongTensor(src).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = create_padding_mask(src_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, 1, src_seq_len]
    
    # 编码源句子
    with torch.no_grad():
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask.squeeze(1)  # [batch_size, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    current_frequency = {}  # 用于记录历史词频
    
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = create_padding_mask(tgt_tensor, pad_idx=tgt_vocab['<pad>'])  # [1, 1, len_tgt]
        
        # 解码器
        with torch.no_grad():
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask.squeeze(1), 
                memory_key_padding_mask=src_padding_mask.squeeze(1)
            )  # [tgt_seq_len, 1, d_model]
            
            # 通过线性层获得logits
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 应用频率惩罚
            penalty = torch.zeros_like(next_token_logits).to(device)
            for word_id, count in current_frequency.items():
                penalty[word_id] = penalty_factor * count
            adjusted_logits = next_token_logits - penalty  # 降低频繁词的logit值
            
            # 计算概率
            probs = F.softmax(adjusted_logits, dim=-1)  # [tgt_vocab_size]
        
        # 选择最大概率的词
        _, next_token = torch.max(probs, dim=-1)  # [1]
        next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
        
        # 更新当前频率
        if next_token in current_frequency:
            current_frequency[next_token] += 1
        else:
            current_frequency[next_token] = 1
    
    # 将目标序列ID转换回词汇
    predicted_sentence = [id_to_tgt_vocab.get(idx, '<unk>') for idx in tgt_indices[1:]]  # 排除<sos>
    return predicted_sentence

# 7. 综合的翻译函数
def translate(model, sentence, src_vocab, tgt_vocab, id_to_tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu', method='greedy', beam_width=3, max_len=5, frequency_penalty=False, penalty_factor=1.0):
    """
    综合的翻译函数，支持多种解码方法
    :param model: 训练好的Transformer模型
    :param sentence: str, 源语言句子
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param device: torch.device
    :param method: str, 'greedy' or 'beam'
    :param beam_width: int, 束宽度，仅当method='beam'时有效
    :param max_len: int, 生成目标序列的最大长度
    :param frequency_penalty: bool, 是否应用频率惩罚
    :param penalty_factor: float, 频率惩罚力度
    :return: str, 生成的翻译句子
    """
    # 数据预处理
    src_ids = tokenize_and_encode(sentence, src_vocab)  # [1, ...]
    
    if method == 'greedy':
        if not frequency_penalty:
            predicted_tokens = greedy_decode(
                model, 
                src_ids, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device
            )
        else:
            predicted_tokens = greedy_decode_with_frequency_penalty(
                model, 
                src_ids, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device, 
                penalty_factor=penalty_factor
            )
    elif method == 'beam':
        predicted_tokens = beam_search_decode(
            model, 
            src_ids, 
            src_vocab, 
            tgt_vocab, 
            id_to_tgt_vocab, 
            beam_width=beam_width, 
            max_len=max_len, 
            device=device
        )
    else:
        raise ValueError("Invalid decoding method. Choose 'greedy' or 'beam'.")
    
    # 后处理
    translation = decode_ids_to_sentence(predicted_tokens, id_to_tgt_vocab)
    return translation

# 8. 模型预测示例
source_sentence = "src_word_1 src_word_2 src_word_3 src_word_4 src_word_5"

# 贪婪搜索
translation_greedy = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5
)
print("Greedy Search Translation:", translation_greedy)

# 束搜索
translation_beam = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='beam',
    beam_width=3,
    max_len=5
)
print("Beam Search Translation:", translation_beam)

# 带频率惩罚的贪婪搜索
translation_penalty = translate(
    model,
    source_sentence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    method='greedy',
    max_len=5,
    frequency_penalty=True,
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", translation_penalty)
```

## **9. 运行结果示例**

假设您的词汇表和模型训练良好，运行上述代码将产生类似以下的输出：

```
Source Vocabulary Size: 64
Target Vocabulary Size: 39
Encoded Source IDs: [1, 4, 5, 6, 7, 2]
Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Beam Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
Frequency Penalty Greedy Search Translation: tgt_word_1 tgt_word_2 tgt_word_3 tgt_word_4 tgt_word_5
```

**注意：**

- 实际输出将取决于您的训练数据和模型参数。
- 如果生成的序列长度不足5或提前生成了 `<eos>`, 生成的翻译句子可能会更短。

## **10. 模型预测的重要优化与注意事项**

### **10.1 处理未登录词（<unk>）**

在分词与编码过程中，对于词汇表中未包含的词汇，确保正确映射到 `<unk>`，避免模型生成随机或无意义的词汇。

```python
ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
```

### **10.2 频率惩罚力度调整**

`penalty_factor` 的值需要根据实际需求和验证集表现进行调整。过高的惩罚力度可能导致模型难以生成常见必要词汇，而过低的力度可能无法有效减少频繁词汇的生成。

### **10.3 设置合理的束宽度（Beam Width）**

束宽度（`beam_width`）决定了搜索过程中保留的候选序列数量。较大的束宽度可以提高翻译质量，但也增加了计算开销。常见的选择范围是3到10。

### **10.4 生成长度控制**

通过设置 `max_len` 参数，可以控制生成序列的最大长度，防止模型生成过长或过短的句子。根据任务需求，调整该参数以获得最佳效果。

### **10.5 模型评估**

在实际应用中，使用自动评估指标（如BLEU、ROUGE）以及人工评估来衡量翻译质量，并根据评估结果进一步优化模型和预测过程。

### **10.6 模型部署**

为了将模型部署到生产环境中，可以考虑以下步骤：

1. **模型导出**：将模型导出为TorchScript或ONNX格式，便于跨平台部署。
2. **API封装**：使用Flask、FastAPI等框架封装预测接口，提供RESTful API服务。
3. **优化加速**：利用GPU加速、模型量化等技术提高预测速度。

**示例：导出模型为TorchScript**

```python
# 创建示例输入
example_src = torch.LongTensor([4, 5, 6, 7, 2]).unsqueeze(0).to(device)  # [1, src_seq_len]
example_tgt = torch.LongTensor([1, 3, 4, 5, 6]).unsqueeze(0).to(device)  # [1, tgt_seq_len]

# 导出为TorchScript
traced_model = torch.jit.trace(model, (example_src, example_tgt))
traced_model.save("transformer_model_traced.pt")
```

## **11. 结论**

通过重新构建词汇表，确保源语言和目标语言有效词汇量分别为60和35，并正确添加特殊标识符，我们已经成功调整了Transformer模型的定义和预测过程。以下是关键步骤的总结：

1. **词汇表构建**：
    - 定义特殊标识符。
    - 创建源语言和目标语言的词汇表，分别包含60和35个有效词，加上特殊标识符。

2. **模型定义调整**：
    - 确保嵌入层的词汇表大小包含所有词汇和特殊标识符。

3. **数据预处理**：
    - 分词、添加特殊标识符并进行编码。
    - 创建填充掩码以处理批量数据中的不等长序列。

4. **预测过程**：
    - 实现贪婪搜索和束搜索，满足不同的生成需求。
    - 集成频率惩罚机制以降低历史中频繁出现的词的概率，促进生成多样性。

5. **后处理**：
    - 将生成的词ID序列转换回实际的词汇，形成可读的翻译句子。

6. **注意事项**：
    - 保持词汇表的一致性。
    - 处理未知词汇。
    - 遇到生成结束符时停止生成。
    - 设置合理的生成长度限制。

通过理解和实施这些步骤，您可以高效地使用训练好的Transformer模型进行机器翻译任务，并根据具体需求进行定制和优化。如果您有更多具体问题或需要进一步的代码示例，欢迎继续提问！