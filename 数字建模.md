了解您的任务目标后，我们将详细介绍如何构建一个基于Transformer的模型来完成以下任务：

- **任务描述**：
  - **输入（源）**：一个长度为5的整数序列，数字范围为[1, 60]，允许重复。
  - **输出（目标）**：一个长度为5的不重复整数序列，数字范围为[1, 35]。
  - **附加要求**：在预测过程中，历史上出现次数较多的目标数字在当前预测时其出现概率应较低，以促进多样化生成。

为实现上述任务，我们将分步完成以下内容：

1. **数据预处理**
2. **模型架构设计**
3. **损失函数设计**
4. **训练过程定义**
5. **预测过程实现**
6. **优化与注意事项**

## **1. 数据预处理**

首先，我们需要对源和目标数据进行预处理，包括词汇表构建、序列编码以及填充。

### **1.1 定义词汇表**

我们将源数字和目标数字视为不同的词汇，因此需要为它们分别构建词汇表，并添加必要的特殊标识符（如 `<pad>`, `<sos>`, `<eos>`, `<unk>`）。

```python
# 定义特殊标识符
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 构建源语言词汇表（1-60）
source_numbers = list(range(1, 61))  # [1, 2, ..., 60]

# 构建目标语言词汇表（1-35）
target_numbers = list(range(1, 36))  # [1, 2, ..., 35]

# 构建源语言词汇表字典
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}  # <pad>:0, <sos>:1, <eos>:2, <unk>:3
src_vocab.update({str(number): idx + len(special_tokens) for idx, number in enumerate(source_numbers)})

# 构建目标语言词汇表字典
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({str(number): idx + len(special_tokens) for idx, number in enumerate(target_numbers)})

# 反向目标词汇表，用于后处理
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

# 检查词汇表大小
print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64 = 4 + 60
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39 = 4 + 35
```

**解释**：

- **源词汇表**：
  - 包含特殊标识符 `<pad>`, `<sos>`, `<eos>`, `<unk>` 分别对应ID 0, 1, 2, 3。
  - 有效词汇为字符串形式的数字 `"1"` 到 `"60"`，对应ID 4 到 63。

- **目标词汇表**：
  - 类似地，特殊标识符对应ID 0, 1, 2, 3。
  - 有效词汇为字符串形式的数字 `"1"` 到 `"35"`，对应ID 4 到 38。

### **1.2 分词与编码函数**

将数字序列转换为对应的ID序列，添加必要的特殊标识符。

```python
def tokenize_and_encode(sequence, vocab, add_sos_eos=False):
    """
    将数字序列转换为ID序列
    :param sequence: list of int, 数字序列
    :param vocab: dict, 词汇表 {word: id}
    :param add_sos_eos: bool, 是否添加 <sos> 和 <eos>
    :return: list of int, 编码后的ID序列
    """
    tokens = [str(num) for num in sequence]
    if add_sos_eos:
        tokens = ['<sos>'] + tokens + ['<eos>']
    ids = [vocab.get(token, vocab['<unk>']) for token in tokens]
    return ids

# 示例
source_sequence = [1, 2, 3, 4, 5]
target_sequence = [10, 20, 30, 40, 50]  # 目标数字范围是1-35，因此需要确保数据有效

# 注意：目标数字50超出范围，需要修正为有效范围内的数字
# 假设正确目标序列
target_sequence = [10, 20, 30, 25, 35]

encoded_src = tokenize_and_encode(source_sequence, src_vocab, add_sos_eos=True)
encoded_tgt = tokenize_and_encode(target_sequence, tgt_vocab, add_sos_eos=True)

print(f"Encoded Source: {encoded_src}")  # [1, id_1, id_2, id_3, id_4, id_5, 2]
print(f"Encoded Target: {encoded_tgt}")  # [1, id_10, id_20, id_30, id_25, id_35, 2]
```

### **1.3 定义Dataset和DataLoader**

使用PyTorch的`Dataset`和`DataLoader`来管理数据，确保批次中的序列具有相同的长度，并应用填充。

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class NumberSequenceDataset(Dataset):
    def __init__(self, source_sequences, target_sequences, src_vocab, tgt_vocab):
        """
        :param source_sequences: list of list of int, 源语言数字序列
        :param target_sequences: list of list of int, 目标语言数字序列
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sequences) == len(target_sequences), "源和目标序列长度不一致"
        self.source_sequences = source_sequences
        self.target_sequences = target_sequences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sequences)
    
    def __getitem__(self, idx):
        src_seq = self.source_sequences[idx]
        tgt_seq = self.target_sequences[idx]
        return src_seq, tgt_seq

def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标输入、目标输出及其掩码
    """
    src_seqs, tgt_seqs = zip(*batch)
    
    # 编码并添加特殊标识符
    encoded_src = [torch.tensor(tokenize_and_encode(seq, src_vocab, add_sos_eos=True), dtype=torch.long) for seq in src_seqs]
    encoded_tgt = [torch.tensor(tokenize_and_encode(seq, tgt_vocab, add_sos_eos=True), dtype=torch.long) for seq in tgt_seqs]
    
    # 填充序列
    src_padded = pad_sequence(encoded_src, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padded = pad_sequence(encoded_tgt, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建目标输入和输出
    tgt_input = tgt_padded[:, :-1]  # [batch_size, tgt_seq_len -1]
    tgt_output = tgt_padded[:, 1:]  # [batch_size, tgt_seq_len -1]
    
    # 创建掩码
    src_padding_mask = (src_padded != src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_input != tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len -1]
    
    return {
        'src': src_padded.to(device),  # [batch_size, src_seq_len]
        'tgt_input': tgt_input.to(device),  # [batch_size, tgt_seq_len -1]
        'tgt_output': tgt_output.to(device),  # [batch_size, tgt_seq_len -1]
        'src_padding_mask': src_padding_mask.to(device),  # [batch_size, src_seq_len]
        'tgt_padding_mask': tgt_padding_mask.to(device)  # [batch_size, tgt_seq_len -1]
    }

# 示例数据
source_sequences = [
    [1, 2, 3, 4, 5],
    [6, 7, 8, 9, 10],
    # ... 更多序列
]

target_sequences = [
    [10, 20, 30, 25, 35],
    [15, 25, 5, 18, 22],
    # ... 更多序列（确保无重复）
]

# 创建Dataset
dataset = NumberSequenceDataset(source_sequences, target_sequences, src_vocab, tgt_vocab)

# 创建DataLoader
batch_size = 32

data_loader = DataLoader(
    dataset, 
    batch_size=batch_size, 
    shuffle=True, 
    collate_fn=collate_fn
)
```

**注意**：

- 确保目标序列中没有重复数字。
- 在实际任务中，替换示例数据为您的真实数据集。

## **2. 模型架构设计**

我们将使用Transformer的编码器-解码器架构。源序列经过编码器处理，解码器生成目标序列。

### **2.1 定义位置编码**

位置编码用来注入位置信息，帮助模型理解序列中词汇的顺序。

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        :param x: Tensor, [batch_size, seq_len, d_model]
        :return: Tensor, 加入位置信息后的输入
        """
        x = x + self.pe[:, :x.size(1)]
        return x
```

### **2.2 定义Transformer模型**

```python
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        """
        生成目标掩码
        :param sz: int, 序列长度
        :return: Tensor, [sz, sz]
        """
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        """
        :param src: Tensor, [batch_size, src_seq_len]
        :param tgt: Tensor, [batch_size, tgt_seq_len]
        :param src_mask: Tensor, [src_seq_len, src_seq_len]
        :param tgt_mask: Tensor, [tgt_seq_len, tgt_seq_len]
        :param src_padding_mask: Tensor, [batch_size, src_seq_len]
        :param tgt_padding_mask: Tensor, [batch_size, tgt_seq_len]
        :param memory_key_padding_mask: Tensor, [batch_size, src_seq_len]
        :return: Tensor, [batch_size, tgt_seq_len, tgt_vocab_size]
        """
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)  # [batch_size, src_seq_len, d_model]
        src_emb = self.pos_encoder(src_emb)
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)  # [batch_size, tgt_seq_len, d_model]
        tgt_emb = self.pos_decoder(tgt_emb)
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits
```

### **模型实例化与参数加载**

```python
# 设定设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 模型参数
src_vocab_size = len(src_vocab)  # 64
tgt_vocab_size = len(tgt_vocab)  # 39
d_model = 512
n_head = 8
num_encoder_layers = 6
num_decoder_layers = 6
dim_feedforward = 2048
dropout = 0.1
max_len = 100

# 实例化模型
model = TransformerModel(
    src_vocab_size=src_vocab_size, 
    tgt_vocab_size=tgt_vocab_size, 
    d_model=d_model, 
    n_head=n_head, 
    num_encoder_layers=num_encoder_layers, 
    num_decoder_layers=num_decoder_layers, 
    dim_feedforward=dim_feedforward, 
    dropout=dropout,
    max_len=max_len
)

# 将模型移动到设备
model = model.to(device)

# 加载预训练模型参数（如果有）
# model.load_state_dict(torch.load('best_model.pt', map_location=device))
# model.eval()
```

## **3. 损失函数设计**

为了满足“历史预测中出现次数多的词，当前预测时概率较小”的要求，我们将设计一个**频率加权交叉熵损失函数**。具体方法是在标准交叉熵损失的基础上，根据目标序列中词汇的历史频率调整损失权重。

### **3.1 频率加权交叉熵损失**

```python
import torch.nn as nn

class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        """
        初始化频率加权交叉熵损失函数
        :param vocab_size: int, 目标词汇表大小
        :param ignore_index: int, 要忽略的标签索引（通常为<pad>）
        :param penalty_factor: float, 频率惩罚系数
        """
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        计算损失
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]，用于计算历史词频
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失
        return loss.mean()
```

**解释**：

- **freq**：计算目标序列中各词的出现频率。
- **weights**：频率的倒数加1，再乘以惩罚系数，防止除以0。
- **loss**：对每个目标词计算交叉熵损失，并乘以对应的权重，降低频繁词的损失贡献。

## **4. 训练过程定义**

接下来，定义训练函数和评价函数，结合频率加权损失进行训练。

### **4.1 定义训练函数**

```python
def train_epoch(model, data_loader, criterion, optimizer, device, clip=1.0):
    """
    训练一个epoch
    :param model: Transformer模型
    :param data_loader: DataLoader，训练数据
    :param criterion: 损失函数
    :param optimizer: 优化器
    :param device: torch.device
    :param clip: float, 梯度裁剪阈值
    :return: float, 平均损失
    """
    model.train()
    epoch_loss = 0
    
    for batch in data_loader:
        src = batch['src']  # [batch_size, src_seq_len]
        tgt_input = batch['tgt_input']  # [batch_size, tgt_seq_len -1]
        tgt_output = batch['tgt_output']  # [batch_size, tgt_seq_len -1]
        src_padding_mask = batch['src_padding_mask']  # [batch_size, src_seq_len]
        tgt_padding_mask = batch['tgt_padding_mask']  # [batch_size, tgt_seq_len -1]
        
        src = src.to(device)
        tgt_input = tgt_input.to(device)
        tgt_output = tgt_output.to(device)
        src_padding_mask = src_padding_mask.to(device)
        tgt_padding_mask = tgt_padding_mask.to(device)
        
        optimizer.zero_grad()
        
        # 前向传播
        logits = model(src, tgt_input)  # [batch_size, tgt_seq_len -1, tgt_vocab_size]
        
        # 调整logits和targets的形状
        logits = logits.view(-1, logits.size(-1))  # [batch_size * (tgt_seq_len -1), tgt_vocab_size]
        tgt_output = tgt_output.view(-1)  # [batch_size * (tgt_seq_len -1)]
        
        # 计算损失
        loss = criterion(logits, tgt_output, tgt_input)  # scalar
        
        # 反向传播
        loss.backward()
        
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)
        
        # 优化器更新
        optimizer.step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(data_loader)
```

### **4.2 定义验证函数**

```python
def evaluate_epoch(model, data_loader, criterion, device):
    """
    评估一个epoch
    :param model: Transformer模型
    :param data_loader: DataLoader，验证数据
    :param criterion: 损失函数
    :param device: torch.device
    :return: float, 平均损失
    """
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in data_loader:
            src = batch['src']
            tgt_input = batch['tgt_input']
            tgt_output = batch['tgt_output']
            src_padding_mask = batch['src_padding_mask']
            tgt_padding_mask = batch['tgt_padding_mask']
            
            src = src.to(device)
            tgt_input = tgt_input.to(device)
            tgt_output = tgt_output.to(device)
            src_padding_mask = src_padding_mask.to(device)
            tgt_padding_mask = tgt_padding_mask.to(device)
            
            # 前向传播
            logits = model(src, tgt_input)
            
            # 调整logits和targets的形状
            logits = logits.view(-1, logits.size(-1))
            tgt_output = tgt_output.view(-1)
            
            # 计算损失
            loss = criterion(logits, tgt_output, tgt_input)
            
            epoch_loss += loss.item()
    
    return epoch_loss / len(data_loader)
```

### **4.3 训练循环**

定义完整的训练循环，包括训练和验证过程，并保存最佳模型。

```python
import torch.optim as optim

def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, clip=1.0, save_path='best_model.pt'):
    """
    训练模型
    :param model: Transformer模型
    :param train_loader: DataLoader，训练数据
    :param val_loader: DataLoader，验证数据
    :param criterion: 损失函数
    :param optimizer: 优化器
    :param device: torch.device
    :param num_epochs: int, 训练轮数
    :param clip: float, 梯度裁剪阈值
    :param save_path: str, 最佳模型保存路径
    :return: None
    """
    best_val_loss = float('inf')
    
    for epoch in range(1, num_epochs + 1):
        print(f"--- Epoch {epoch} ---")
        
        # 训练
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, clip)
        print(f"Train Loss: {train_loss:.4f}")
        
        # 验证
        if val_loader is not None:
            val_loss = evaluate_epoch(model, val_loader, criterion, device)
            print(f"Validation Loss: {val_loss:.4f}")
            
            # 保存最佳模型
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), save_path)
                print(f"Saved Best Model with Validation Loss: {best_val_loss:.4f}")
        else:
            # 如果没有验证集，选择训练损失最小的模型
            if train_loss < best_val_loss:
                best_val_loss = train_loss
                torch.save(model.state_dict(), save_path)
                print(f"Saved Best Model with Train Loss: {best_val_loss:.4f}")
```

**使用示例**：

```python
# 定义优化器
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# 训练模型（假设有验证集 DataLoader）
# train_model(model, data_loader, val_data_loader, criterion, optimizer, device, num_epochs=10, clip=1.0, save_path='best_model.pt')

# 如果没有验证集，传入 None
train_model(model, data_loader, None, criterion, optimizer, device, num_epochs=10, clip=1.0, save_path='best_model.pt')
```

**注意**：

- 若有验证集，应提供`val_data_loader`以监控模型性能并保存最佳参数。
- 调整超参数如`learning rate`、`num_epochs`、`penalty_factor`等，依据具体任务需求和数据集特点。

## **5. 预测过程实现**

训练完成后，使用模型进行预测。我们将实现两种解码策略：

1. **贪婪搜索（Greedy Search）**
2. **束搜索（Beam Search）**

同时，我们将整合**频率惩罚机制**，以降低历史中频繁出现的目标数字的预测概率，促进多样化生成。

### **5.1 贪婪搜索（Greedy Search）**

在每一步选择概率最高的词作为当前预测的词。

```python
def greedy_decode(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用贪婪搜索进行预测
    :param model: 训练好的Transformer模型
    :param src_seq: list of int, 源数字序列的ID
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of int, 预测的目标数字ID序列
    """
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
        
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        with torch.no_grad():
            # 解码器生成
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 线性层
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 选择最大概率的词
            _, next_token = torch.max(next_token_logits, dim=-1)  # [1]
            next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
    
    # 移除<sos>并转换为最终序列
    predicted_ids = tgt_indices[1:]  # 排除<sos>
    return predicted_ids
```

### **5.2 束搜索（Beam Search）**

束搜索在每一步保留多个（beam width）最有可能的候选序列，以找到更优的翻译结果。

```python
class BeamSearchNode:
    def __init__(self, previous_node, word_id, log_prob, length):
        self.previous_node = previous_node  # 上一个节点
        self.word_id = word_id              # 当前词ID
        self.log_prob = log_prob            # 累计对数概率
        self.length = length                # 序列长度
    
    def __lt__(self, other):
        return self.log_prob < other.log_prob

def beam_search_decode(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, beam_width=3, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    使用束搜索进行预测
    :param model: 训练好的Transformer模型
    :param src_seq: list of int, 源数字序列的ID
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param beam_width: int, 束宽度
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :return: list of int, 最佳预测的目标数字ID序列
    """
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
    
    # 初始化束
    BOS = tgt_vocab['<sos>']
    EOS = tgt_vocab['<eos>']
    candidates = []
    root = BeamSearchNode(previous_node=None, word_id=BOS, log_prob=0.0, length=1)
    candidates.append(root)
    
    # 初始化结束节点
    end_nodes = []
    
    for _ in range(max_len):
        all_candidates = []
        for candidate in candidates:
            if candidate.word_id == EOS:
                end_nodes.append(candidate)
                continue
            
            # 当前目标序列
            tgt_indices = []
            current = candidate
            while current is not None:
                tgt_indices.append(current.word_id)
                current = current.previous_node
            tgt_indices = tgt_indices[::-1]
            
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
            
            # 创建目标掩码
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
            
            # 创建目标填充掩码
            tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
            
            with torch.no_grad():
                # 解码器生成
                output = model.transformer.decoder(
                    model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                    memory, 
                    tgt_mask=tgt_mask, 
                    tgt_key_padding_mask=tgt_padding_mask, 
                    memory_key_padding_mask=src_padding_mask
                )  # [tgt_seq_len, 1, d_model]
                
                # 线性层
                logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
                
                # 获取最后一个时间步的logit
                next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
                
                # 计算对数概率
                log_probs = F.log_softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 获取前beam_width个词
            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)
            
            for i in range(beam_width):
                next_word_id = topk_indices[i].item()
                log_prob = log_probs[next_word_id].item()
                new_node = BeamSearchNode(previous_node=candidate, word_id=next_word_id, log_prob=candidate.log_prob + log_prob, length=candidate.length + 1)
                all_candidates.append(new_node)
        
        # 排序并选择beam_width个最好的候选
        ordered = sorted(all_candidates, key=lambda x: x.log_prob, reverse=True)
        candidates = ordered[:beam_width]
        
        # 如果所有候选生成了EOS，停止
        if len(end_nodes) >= beam_width:
            break
    
    # 如果没有结束节点，则将当前候选添加进去
    if len(end_nodes) == 0:
        end_nodes = candidates
    
    # 选择概率最高的结束节点
    end_nodes = sorted(end_nodes, key=lambda x: x.log_prob, reverse=True)
    best_node = end_nodes[0]
    
    # 回溯生成序列
    tgt_indices = []
    current = best_node
    while current is not None:
        tgt_indices.append(current.word_id)
        current = current.previous_node
    tgt_indices = tgt_indices[::-1]
    
    # 移除<sos>并终止于<eos>
    if tgt_indices[0] == BOS:
        tgt_indices = tgt_indices[1:]
    if EOS in tgt_indices:
        tgt_indices = tgt_indices[:tgt_indices.index(EOS)]
    
    return tgt_indices
```

### **5.3 频率惩罚机制**

在解码过程中，动态调整词汇的logits，降低历史中频繁出现的词汇的预测概率。

```python
def greedy_decode_with_frequency_penalty(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu', penalty_factor=1.0):
    """
    使用贪婪搜索并结合频率惩罚进行预测
    :param model: 训练好的Transformer模型
    :param src_seq: list of int, 源数字序列的ID
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :param penalty_factor: float, 频率惩罚力度
    :return: list of int, 预测的目标数字ID序列
    """
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    current_frequency = {}  # 记录已生成的词频
    
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        with torch.no_grad():
            # 解码器生成
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 线性层
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 计算概率
            probs = F.softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 应用频率惩罚
            penalty = torch.zeros_like(probs).to(device)
            for word_id, count in current_frequency.items():
                penalty[word_id] = penalty_factor * count
            adjusted_probs = probs - penalty  # 降低频繁词的概率
            
            # 再次计算概率
            adjusted_probs = F.softmax(adjusted_probs, dim=-1)  # [tgt_vocab_size]
        
        # 选择最大概率的词
        _, next_token = torch.max(adjusted_probs, dim=-1)  # [1]
        next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
        
        # 更新当前频率
        if next_token in current_frequency:
            current_frequency[next_token] += 1
        else:
            current_frequency[next_token] = 1
    
    # 移除<sos>并转换为最终序列
    predicted_ids = tgt_indices[1:]  # 排除<sos>
    return predicted_ids
```

### **5.4 综合预测函数**

将各种解码策略集成到一个函数中，以便灵活选择。

```python
def decode_ids_to_sequence(ids, id_to_vocab):
    """
    将预测的ID序列转换为数字序列
    :param ids: list of int, 预测的目标数字ID序列
    :param id_to_vocab: dict, 目标语言反向词汇表 {id: word}
    :return: list of int, 预测的目标数字序列
    """
    tokens = [id_to_vocab.get(idx, '<unk>') for idx in ids]
    # 将字符串形式的数字转回整数
    numbers = []
    for token in tokens:
        if token.isdigit():
            numbers.append(int(token))
        else:
            numbers.append(token)  # 或者选择其他处理方式
    return numbers

def translate(model, sentence, src_vocab, tgt_vocab, id_to_tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu', method='greedy', beam_width=3, max_len=5, frequency_penalty=False, penalty_factor=1.0):
    """
    综合的翻译函数，支持多种解码方法
    :param model: 训练好的Transformer模型
    :param sentence: list of int, 源数字序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param device: torch.device
    :param method: str, 'greedy' 或 'beam'
    :param beam_width: int, 束宽度，仅当method='beam'时有效
    :param max_len: int, 生成目标序列的最大长度
    :param frequency_penalty: bool, 是否应用频率惩罚
    :param penalty_factor: float, 频率惩罚力度
    :return: list of int, 生成的目标数字序列
    """
    if method == 'greedy':
        if frequency_penalty:
            predicted_ids = greedy_decode_with_frequency_penalty(
                model, 
                sentence, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device, 
                penalty_factor=penalty_factor
            )
        else:
            predicted_ids = greedy_decode(
                model, 
                sentence, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device
            )
    elif method == 'beam':
        predicted_ids = beam_search_decode(
            model, 
            sentence, 
            src_vocab, 
            tgt_vocab, 
            id_to_tgt_vocab, 
            beam_width=beam_width, 
            max_len=max_len, 
            device=device
        )
    else:
        raise ValueError("Invalid decoding method. Choose 'greedy' or 'beam'.")
    
    return decode_ids_to_sequence(predicted_ids, id_to_tgt_vocab)
```

**使用示例**：

```python
# 定义源序列和目标序列
source_sequence = [1, 2, 3, 4, 5]  # 源数字序列
target_sequence = [10, 20, 30, 25, 35]  # 目标数字序列（无重复）

# 编码源序列
encoded_src = tokenize_and_encode(source_sequence, src_vocab, add_sos_eos=True)
print(f"Encoded Source: {encoded_src}")

# 训练数据示例
# 请确保训练数据符合源序列和目标序列的定义，并且目标序列无重复数字
```

**预测**

```python
# 预测使用贪婪搜索
translation_greedy = translate(
    model,
    source_sequence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    device=device,
    method='greedy',
    max_len=5,
    frequency_penalty=False
)
print("Greedy Search Translation:", translation_greedy)

# 预测使用束搜索
translation_beam = translate(
    model,
    source_sequence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    device=device,
    method='beam',
    beam_width=3,
    max_len=5
)
print("Beam Search Translation:", translation_beam)

# 预测使用带频率惩罚的贪婪搜索
translation_penalty = translate(
    model,
    source_sequence,
    src_vocab,
    tgt_vocab,
    id_to_tgt_vocab,
    device=device,
    method='greedy',
    max_len=5,
    frequency_penalty=True,
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", translation_penalty)
```

**输出示例**：

```
Encoded Source: [1, 4, 5, 6, 7, 2]
Greedy Search Translation: [10, 20, 30, 25, 35]
Beam Search Translation: [10, 20, 30, 25, 35]
Frequency Penalty Greedy Search Translation: [10, 20, 30, 25, 35]
```

**注意**：

- 实际输出取决于您的训练数据和模型参数。
- 确保源和目标序列数字在词汇表中正确映射，避免生成`<unk>`。

## **6. 优化与注意事项**

### **6.1 确保目标序列无重复**

在训练数据准备时，确保目标序列中的数字不重复。如果存在重复目标序列，模型可能会学到不符合要求的预测。

### **6.2 频率惩罚参数调整**

- **`penalty_factor`**：控制频率惩罚力度。值越大，频繁出现的数字其预测概率越低。需要根据实际效果进行调整。
- 观察生成结果，确保模型既能准确预测目标序列，又能遵循无重复和频率惩罚的要求。

### **6.3 验证与测试**

- **验证集**：使用独立的验证集监控模型性能，防止过拟合。
- **评价指标**：可以采用准确率（准确预测整个序列）、召回率（部分数字预测正确）、序列级别BLEU等指标评估模型表现。

### **6.4 模型部署**

- **模型保存**：在训练过程中定期保存最佳模型参数，以备后续加载和部署。
- **推理优化**：在部署阶段，可以使用TorchScript或ONNX将模型转换为更高效的推理格式。
- **硬件加速**：利用GPU加速预测过程，特别是在需要实时或大规模预测的场景下。

### **6.5 处理边界情况**

- **序列长度**：虽然当前任务规定序列长度为5，但在实际应用中可能需要处理不同长度的序列。适当调整`max_len`和模型配置。
- **未知数字**：确保在源序列中遇到未知数字时，正确映射为`<unk>`。在目标序列中不存在未知数字，因为输出范围固定。

## **7. 完整代码示例**

以下是一个整合以上所有步骤的完整代码示例，适用于您的任务定义。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
import random
import numpy as np
import heapq

# 1. 设置随机种子
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# 2. 定义特殊标识符和词汇表
special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']

# 源语言有效词汇（1-60）
source_numbers = list(range(1, 61))  # [1, 2, ..., 60]

# 目标语言有效词汇（1-35）
target_numbers = list(range(1, 36))  # [1, 2, ..., 35]

# 构建源语言词汇表
src_vocab = {token: idx for idx, token in enumerate(special_tokens)}
src_vocab.update({str(number): idx + len(special_tokens) for idx, number in enumerate(source_numbers)})

# 构建目标语言词汇表
tgt_vocab = {token: idx for idx, token in enumerate(special_tokens)}
tgt_vocab.update({str(number): idx + len(special_tokens) for idx, number in enumerate(target_numbers)})

# 反向目标词汇表
id_to_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}

print(f"Source Vocabulary Size: {len(src_vocab)}")  # 64
print(f"Target Vocabulary Size: {len(tgt_vocab)}")  # 39

# 3. 定义位置编码
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]

        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

# 4. 定义Transformer模型
class TransformerModel(nn.Module):
    def __init__(
        self, 
        src_vocab_size, 
        tgt_vocab_size, 
        d_model=512, 
        n_head=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6, 
        dim_feedforward=2048, 
        dropout=0.1,
        max_len=100
    ):
        super(TransformerModel, self).__init__()
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_head,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz)) == 1
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(
        self, 
        src, 
        tgt, 
        src_mask=None, 
        tgt_mask=None, 
        src_padding_mask=None, 
        tgt_padding_mask=None, 
        memory_key_padding_mask=None
    ):
        src_emb = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)  # [batch_size, src_seq_len, d_model]
        src_emb = self.pos_encoder(src_emb)
        
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)  # [batch_size, tgt_seq_len, d_model]
        tgt_emb = self.pos_decoder(tgt_emb)
        
        # Transformer需要 [seq_len, batch_size, d_model] 输入
        output = self.transformer(
            src_emb.transpose(0, 1),  # [src_seq_len, batch_size, d_model]
            tgt_emb.transpose(0, 1),  # [tgt_seq_len, batch_size, d_model]
            src_mask,
            tgt_mask,
            None,
            src_padding_mask,
            tgt_padding_mask,
            memory_key_padding_mask
        )  # [tgt_seq_len, batch_size, d_model]
        
        output = output.transpose(0, 1)  # [batch_size, tgt_seq_len, d_model]
        logits = self.fc_out(output)  # [batch_size, tgt_seq_len, tgt_vocab_size]
        return logits

# 5. 定义频率加权交叉熵损失
class FrequencyWeightedCrossEntropyLoss(nn.Module):
    def __init__(self, vocab_size, ignore_index=-100, penalty_factor=1.0):
        """
        初始化频率加权交叉熵损失函数
        :param vocab_size: int, 目标词汇表大小
        :param ignore_index: int, 要忽略的标签索引（通常为<pad>）
        :param penalty_factor: float, 频率惩罚系数
        """
        super(FrequencyWeightedCrossEntropyLoss, self).__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        self.penalty_factor = penalty_factor
        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='none')
    
    def forward(self, logits, targets, history):
        """
        计算损失
        :param logits: Tensor, [batch_size * seq_len, vocab_size]
        :param targets: Tensor, [batch_size * seq_len]
        :param history: Tensor, [batch_size, seq_len]，用于计算历史词频
        :return: Tensor, scalar loss
        """
        # 计算每个词在历史中的频率
        history_flat = history.view(-1)
        freq = torch.bincount(history_flat, minlength=self.vocab_size).float()
        freq = freq.to(logits.device)
        
        # 计算权重（频率的倒数）
        weights = 1.0 / (1.0 + self.penalty_factor * freq)  # [vocab_size]
        
        # 获取目标词的权重
        target_weights = weights[targets]  # [batch_size * seq_len]
        
        # 计算交叉熵损失
        loss = self.criterion(logits, targets)  # [batch_size * seq_len]
        
        # 应用权重
        loss = loss * target_weights
        
        # 计算平均损失
        return loss.mean()

# 6. 定义Dataset和DataLoader
class NumberSequenceDataset(Dataset):
    def __init__(self, source_sequences, target_sequences, src_vocab, tgt_vocab):
        """
        :param source_sequences: list of list of int, 源语言数字序列
        :param target_sequences: list of list of int, 目标语言数字序列
        :param src_vocab: dict, 源语言词汇表 {word: id}
        :param tgt_vocab: dict, 目标语言词汇表 {word: id}
        """
        assert len(source_sequences) == len(target_sequences), "源和目标序列长度不一致"
        self.source_sequences = source_sequences
        self.target_sequences = target_sequences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
    
    def __len__(self):
        return len(self.source_sequences)
    
    def __getitem__(self, idx):
        src_seq = self.source_sequences[idx]
        tgt_seq = self.target_sequences[idx]
        return src_seq, tgt_seq

def collate_fn(batch):
    """
    :param batch: list of tuples (src_seq, tgt_seq)
    :return: dict, 包含填充后的源序列、目标输入、目标输出及其掩码
    """
    src_seqs, tgt_seqs = zip(*batch)
    
    # 编码并添加特殊标识符
    encoded_src = [torch.tensor(tokenize_and_encode(seq, src_vocab, add_sos_eos=True), dtype=torch.long) for seq in src_seqs]
    encoded_tgt = [torch.tensor(tokenize_and_encode(seq, tgt_vocab, add_sos_eos=True), dtype=torch.long) for seq in tgt_seqs]
    
    # 填充序列
    src_padded = pad_sequence(encoded_src, batch_first=True, padding_value=src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padded = pad_sequence(encoded_tgt, batch_first=True, padding_value=tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len]
    
    # 创建目标输入和输出
    tgt_input = tgt_padded[:, :-1]  # [batch_size, tgt_seq_len -1]
    tgt_output = tgt_padded[:, 1:]  # [batch_size, tgt_seq_len -1]
    
    # 创建掩码
    src_padding_mask = (src_padded != src_vocab['<pad>'])  # [batch_size, src_seq_len]
    tgt_padding_mask = (tgt_input != tgt_vocab['<pad>'])  # [batch_size, tgt_seq_len -1]
    
    return {
        'src': src_padded.to(device),  # [batch_size, src_seq_len]
        'tgt_input': tgt_input.to(device),  # [batch_size, tgt_seq_len -1]
        'tgt_output': tgt_output.to(device),  # [batch_size, tgt_seq_len -1]
        'src_padding_mask': src_padding_mask.to(device),  # [batch_size, src_seq_len]
        'tgt_padding_mask': tgt_padding_mask.to(device)  # [batch_size, tgt_seq_len -1]
    }

# 7. 定义贪婪搜索和束搜索解码函数

# 贪婪搜索
def greedy_decode(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        with torch.no_grad():
            # 解码器生成
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 线性层
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 选择最大概率的词
            _, next_token = torch.max(next_token_logits, dim=-1)  # [1]
            next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
    
    # 移除 <sos> 并转换为最终序列
    predicted_ids = tgt_indices[1:]  # 排除 <sos>
    return predicted_ids

# 束搜索
class BeamSearchNode:
    def __init__(self, previous_node, word_id, log_prob, length):
        self.previous_node = previous_node  # 上一个节点
        self.word_id = word_id              # 当前词ID
        self.log_prob = log_prob            # 累计对数概率
        self.length = length                # 序列长度
    
    def __lt__(self, other):
        return self.log_prob < other.log_prob

def beam_search_decode(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, beam_width=3, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
    
    # 初始化束
    BOS = tgt_vocab['<sos>']
    EOS = tgt_vocab['<eos>']
    candidates = []
    root = BeamSearchNode(previous_node=None, word_id=BOS, log_prob=0.0, length=1)
    candidates.append(root)
    
    # 存储完整的句子
    end_nodes = []
    
    for _ in range(max_len):
        all_candidates = []
        for candidate in candidates:
            if candidate.word_id == EOS:
                end_nodes.append(candidate)
                continue
            
            # 当前目标序列
            tgt_indices = []
            current = candidate
            while current is not None:
                tgt_indices.append(current.word_id)
                current = current.previous_node
            tgt_indices = tgt_indices[::-1]
            
            tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
            
            # 创建目标掩码
            tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
            
            # 创建目标填充掩码
            tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
            
            with torch.no_grad():
                # 解码器生成
                output = model.transformer.decoder(
                    model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                    memory, 
                    tgt_mask=tgt_mask, 
                    tgt_key_padding_mask=tgt_padding_mask, 
                    memory_key_padding_mask=src_padding_mask
                )  # [tgt_seq_len, 1, d_model]
                
                # 线性层
                logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
                
                # 获取最后一个时间步的logit
                next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
                
                # 计算对数概率
                log_probs = F.log_softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 获取前beam_width个词
            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)
            
            for i in range(beam_width):
                next_word_id = topk_indices[i].item()
                log_prob = log_probs[next_word_id].item()
                new_node = BeamSearchNode(
                    previous_node=candidate, 
                    word_id=next_word_id,
                    log_prob=candidate.log_prob + log_prob,
                    length=candidate.length + 1
                )
                all_candidates.append(new_node)
        
        # 选择beam_width个最好的候选
        ordered = sorted(all_candidates, key=lambda x: x.log_prob, reverse=True)
        candidates = ordered[:beam_width]
        
        # 如果所有候选生成了EOS，停止
        if len(candidates) == 0:
            break
    
    # 添加所有结束节点
    end_nodes += candidates
    
    # 选择概率最高的结束节点
    end_nodes = sorted(end_nodes, key=lambda x: x.log_prob, reverse=True)
    best_node = end_nodes[0]
    
    # 回溯生成序列
    tgt_indices = []
    current = best_node
    while current is not None:
        tgt_indices.append(current.word_id)
        current = current.previous_node
    tgt_indices = tgt_indices[::-1]
    
    # 移除 <sos> 并终止于 <eos>
    if tgt_indices[0] == BOS:
        tgt_indices = tgt_indices[1:]
    if EOS in tgt_indices:
        tgt_indices = tgt_indices[:tgt_indices.index(EOS)]
    
    return tgt_indices

# 5.5 定义频率惩罚贪婪搜索
def greedy_decode_with_frequency_penalty(model, src_seq, src_vocab, tgt_vocab, id_to_tgt_vocab, max_len=5, device='cuda' if torch.cuda.is_available() else 'cpu', penalty_factor=1.0):
    """
    使用贪婪搜索并结合频率惩罚进行预测
    :param model: 训练好的Transformer模型
    :param src_seq: list of int, 源数字序列的ID
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param max_len: int, 生成目标序列的最大长度
    :param device: torch.device
    :param penalty_factor: float, 频率惩罚力度
    :return: list of int, 预测的目标数字ID序列
    """
    model.eval()
    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)  # [1, src_seq_len]
    
    # 创建源填充掩码
    src_padding_mask = (src_tensor != src_vocab['<pad>']).to(device)  # [1, src_seq_len]
    
    with torch.no_grad():
        # 编码源序列
        memory = model.transformer.encoder(
            model.pos_encoder(model.src_embedding(src_tensor) * math.sqrt(model.src_embedding.embedding_dim)).transpose(0,1),  # [src_seq_len, 1, d_model]
            src_key_padding_mask=src_padding_mask  # [1, src_seq_len]
        )
    
    # 初始化目标序列
    tgt_indices = [tgt_vocab['<sos>']]
    current_frequency = {}  # 记录已生成的词频
    
    for _ in range(max_len):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)  # [1, len_tgt]
        
        # 创建目标掩码
        tgt_mask = model.generate_square_subsequent_mask(len(tgt_indices)).to(device)  # [len_tgt, len_tgt]
        
        # 创建目标填充掩码
        tgt_padding_mask = (tgt_tensor != tgt_vocab['<pad>']).to(device)  # [1, len_tgt]
        
        with torch.no_grad():
            # 解码器生成
            output = model.transformer.decoder(
                model.pos_decoder(model.tgt_embedding(tgt_tensor) * math.sqrt(model.tgt_embedding.embedding_dim)).transpose(0,1), 
                memory, 
                tgt_mask=tgt_mask, 
                tgt_key_padding_mask=tgt_padding_mask, 
                memory_key_padding_mask=src_padding_mask
            )  # [tgt_seq_len, 1, d_model]
            
            # 线性层
            logits = model.fc_out(output.transpose(0,1))  # [1, tgt_seq_len, tgt_vocab_size]
            
            # 获取最后一个时间步的logit
            next_token_logits = logits[0, -1, :]  # [tgt_vocab_size]
            
            # 计算概率
            probs = F.softmax(next_token_logits, dim=-1)  # [tgt_vocab_size]
            
            # 应用频率惩罚
            penalty = torch.zeros_like(probs).to(device)
            for word_id, count in current_frequency.items():
                penalty[word_id] = penalty_factor * count
            adjusted_probs = probs - penalty  # 降低频繁词的概率
            
            # 再次计算概率
            adjusted_probs = F.softmax(adjusted_probs, dim=-1)  # [tgt_vocab_size]
        
        # 选择最大概率的词
        _, next_token = torch.max(adjusted_probs, dim=-1)  # [1]
        next_token = next_token.item()
        
        # 如果生成结束符，停止
        if next_token == tgt_vocab['<eos>']:
            break
        
        # 添加到目标序列
        tgt_indices.append(next_token)
        
        # 更新当前频率
        if next_token in current_frequency:
            current_frequency[next_token] += 1
        else:
            current_frequency[next_token] = 1
    
    # 移除 <sos> 并转换为最终序列
    predicted_ids = tgt_indices[1:]  # 排除 <sos>
    return predicted_ids
```

### **5.6 综合翻译函数**

```python
def translate(model, sentence, src_vocab, tgt_vocab, id_to_tgt_vocab, device='cuda' if torch.cuda.is_available() else 'cpu', method='greedy', beam_width=3, max_len=5, frequency_penalty=False, penalty_factor=1.0):
    """
    综合的翻译函数，支持多种解码方法
    :param model: 训练好的Transformer模型
    :param sentence: list of int, 源数字序列
    :param src_vocab: dict, 源语言词汇表 {word: id}
    :param tgt_vocab: dict, 目标语言词汇表 {word: id}
    :param id_to_tgt_vocab: dict, 目标语言反向词汇表 {id: word}
    :param device: torch.device
    :param method: str, 'greedy' 或 'beam'
    :param beam_width: int, 束宽度，仅当method='beam'时有效
    :param max_len: int, 生成目标序列的最大长度
    :param frequency_penalty: bool, 是否应用频率惩罚
    :param penalty_factor: float, 频率惩罚力度
    :return: list of int, 生成的目标数字序列
    """
    if method == 'greedy':
        if frequency_penalty:
            predicted_ids = greedy_decode_with_frequency_penalty(
                model, 
                sentence, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device, 
                penalty_factor=penalty_factor
            )
        else:
            predicted_ids = greedy_decode(
                model, 
                sentence, 
                src_vocab, 
                tgt_vocab, 
                id_to_tgt_vocab, 
                max_len=max_len, 
                device=device
            )
    elif method == 'beam':
        predicted_ids = beam_search_decode(
            model, 
            sentence, 
            src_vocab, 
            tgt_vocab, 
            id_to_tgt_vocab, 
            beam_width=beam_width, 
            max_len=max_len, 
            device=device
        )
    else:
        raise ValueError("Invalid decoding method. Choose 'greedy' or 'beam'.")
    
    return predicted_ids

def decode_ids_to_sequence(ids, id_to_vocab):
    """
    将预测的ID序列转换为数字序列
    :param ids: list of int, 预测的目标数字ID序列
    :param id_to_vocab: dict, 目标语言反向词汇表 {id: word}
    :return: list of int, 生成的目标数字序列
    """
    tokens = [id_to_vocab.get(idx, '<unk>') for idx in ids]
    # 将字符串形式的数字转回整数
    numbers = []
    for token in tokens:
        if token.isdigit():
            numbers.append(int(token))
        else:
            numbers.append(token)  # 或者选择其他处理方式
    return numbers
```

**使用示例**：

```python
# 示例源序列
source_sequence = [1, 2, 3, 4, 5]

# 编码源序列
encoded_src = tokenize_and_encode(source_sequence, src_vocab, add_sos_eos=True)
print(f"Encoded Source: {encoded_src}")  # [1, 4, 5, 6, 7, 2]

# 预测使用贪婪搜索
predicted_ids_greedy = translate(
    model, 
    source_sequence, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    device=device,
    method='greedy',
    max_len=5,
    frequency_penalty=False
)
print("Greedy Search Translation:", predicted_ids_greedy)  # [10, 20, 30, 25, 35]

# 预测使用束搜索
predicted_ids_beam = translate(
    model, 
    source_sequence, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    device=device,
    method='beam',
    beam_width=3,
    max_len=5
)
print("Beam Search Translation:", predicted_ids_beam)  # [10, 20, 30, 25, 35]

# 预测使用带频率惩罚的贪婪搜索
predicted_ids_penalty = translate(
    model, 
    source_sequence, 
    src_vocab, 
    tgt_vocab, 
    id_to_tgt_vocab, 
    device=device,
    method='greedy',
    max_len=5,
    frequency_penalty=True,
    penalty_factor=1.0
)
print("Frequency Penalty Greedy Search Translation:", predicted_ids_penalty)  # [10, 20, 30, 25, 35]
```

**注意**：

- 确保源和目标序列中的数字均在词汇表定义范围内。
- 若目标序列中包含重复数字，模型可能无法正确学习无重复的预测要求。

## **总结**

通过上述详细步骤，我们成功调整了训练过程，以适应以下任务需求：

- **输入**：长度为5的数字序列，范围[1, 60]，允许重复。
- **输出**：长度为5的不重复数字序列，范围[1, 35]。
- **附加要求**：生成过程中，历史中频繁出现的数字概率应降低，促进多样化。

**关键要点**：

1. **词汇表构建**：
   - 分别为源语言和目标语言构建包含特殊标识符的词汇表。
   
2. **数据预处理**：
   - 编码输入输出序列，确保目标序列中无重复数字。
   - 使用`DataLoader`管理批次数据，应用填充和掩码。
   
3. **模型架构**：
   - 使用Transformer的编码器-解码器架构，调整嵌入层和输出层以匹配词汇表大小。
   
4. **损失函数**：
   - 设计频率加权交叉熵损失，通过历史词频调整损失权重，降低频繁词的预测概率。
   
5. **训练过程**：
   - 实施训练和验证循环，结合损失函数和优化器进行模型参数更新。
   - 使用梯度裁剪防止梯度爆炸。
   
6. **预测过程**：
   - 实现贪婪搜索和束搜索解码策略。
   - 在预测过程中，应用频率惩罚机制，动态调整预测概率。
   
7. **优化与部署**：
   - 根据验证集评估模型性能，调整超参数如学习率、束宽度和频率惩罚力度。
   - 保存最佳模型参数，便于后续加载和部署。
   - 确保在部署阶段处理未知数字和序列生成的边界情况。

通过以上步骤，您可以成功构建、训练并使用Transformer模型完成指定的数字序列预测任务。如在实施过程中遇到任何问题或需要进一步帮助，欢迎继续提问！